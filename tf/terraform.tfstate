{
  "version": 4,
  "terraform_version": "1.5.5",
  "serial": 20,
  "lineage": "5edb7d02-1f5e-f9d4-b751-d27ba488e424",
  "outputs": {},
  "resources": [
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "jenkins",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "./charts/jenkins",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "jenkins",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.414.1",
                "chart": "jenkins",
                "name": "jenkins",
                "namespace": "jenkins",
                "revision": 1,
                "values": "{\"additionalAgents\":{},\"agent\":{\"TTYEnabled\":false,\"additionalContainers\":[],\"alwaysPullImage\":false,\"annotations\":{},\"args\":\"${computer.jnlpmac} ${computer.name}\",\"command\":null,\"componentName\":\"jenkins-agent\",\"connectTimeout\":100,\"containerCap\":10,\"customJenkinsLabels\":[],\"defaultsProviderTemplate\":\"\",\"directConnection\":false,\"disableDefaultAgent\":false,\"enabled\":true,\"envVars\":[],\"hostNetworking\":false,\"idleMinutes\":0,\"image\":\"jenkins/inbound-agent\",\"imagePullSecretName\":null,\"jenkinsTunnel\":null,\"jenkinsUrl\":null,\"jnlpregistry\":null,\"kubernetesConnectTimeout\":5,\"kubernetesReadTimeout\":15,\"livenessProbe\":{},\"maxRequestsPerHostStr\":\"32\",\"namespace\":null,\"nodeSelector\":{},\"nodeUsageMode\":\"NORMAL\",\"podName\":\"default\",\"podRetention\":\"Never\",\"podTemplates\":{},\"privileged\":false,\"resources\":{\"limits\":{\"cpu\":\"512m\",\"memory\":\"512Mi\"},\"requests\":{\"cpu\":\"512m\",\"memory\":\"512Mi\"}},\"runAsGroup\":null,\"runAsUser\":null,\"secretEnvVars\":[],\"showRawYaml\":true,\"sideContainerName\":\"jnlp\",\"tag\":\"3107.v665000b_51092-15\",\"volumes\":[],\"websocket\":false,\"workingDir\":\"/home/jenkins/agent\",\"workspaceVolume\":{},\"yamlMergeStrategy\":\"override\",\"yamlTemplate\":\"\"},\"awsSecurityGroupPolicies\":{\"enabled\":false,\"policies\":[{\"name\":\"\",\"podSelector\":{},\"securityGroupIds\":[]}]},\"backup\":{\"activeDeadlineSeconds\":\"\",\"componentName\":\"backup\",\"destination\":\"s3://jenkins-data/backup\",\"enabled\":false,\"env\":[],\"existingSecret\":{},\"extraArgs\":[],\"fsGroup\":1000,\"image\":{\"repository\":\"maorfr/kube-tasks\",\"tag\":\"0.2.0\"},\"imagePullSecretName\":null,\"labels\":{},\"onlyJobs\":false,\"resources\":{\"limits\":{\"cpu\":1,\"memory\":\"1Gi\"},\"requests\":{\"cpu\":1,\"memory\":\"1Gi\"}},\"runAsUser\":1000,\"schedule\":\"0 2 * * *\",\"securityContextCapabilities\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":null},\"usePodSecurityContext\":true},\"checkDeprecation\":true,\"clusterZone\":\"cluster.local\",\"controller\":{\"JCasC\":{\"authorizationStrategy\":\"loggedInUsersCanDoAnything:\\n  allowAnonymousRead: false\",\"configScripts\":{},\"configUrls\":[],\"defaultConfig\":true,\"security\":{\"apiToken\":{\"creationOfLegacyTokenEnabled\":false,\"tokenGenerationOnCreationEnabled\":false,\"usageStatisticsEnabled\":true}},\"securityRealm\":\"local:\\n  allowsSignup: false\\n  enableCaptcha: false\\n  users:\\n  - id: \\\"${chart-admin-username}\\\"\\n    name: \\\"Jenkins Admin\\\"\\n    password: \\\"${chart-admin-password}\\\"\"},\"additionalExistingSecrets\":[],\"additionalPlugins\":[],\"additionalSecrets\":[],\"admin\":{\"existingSecret\":\"\",\"passwordKey\":\"jenkins-admin-password\",\"userKey\":\"jenkins-admin-user\"},\"adminPassword\":\"admin\",\"adminSecret\":true,\"adminUser\":\"admin\",\"affinity\":{},\"agentListenerEnabled\":true,\"agentListenerExternalTrafficPolicy\":null,\"agentListenerHostPort\":null,\"agentListenerLoadBalancerIP\":null,\"agentListenerLoadBalancerSourceRanges\":[\"0.0.0.0/0\"],\"agentListenerNodePort\":null,\"agentListenerPort\":50000,\"agentListenerServiceAnnotations\":{},\"agentListenerServiceType\":\"ClusterIP\",\"backendconfig\":{\"annotations\":{},\"apiVersion\":\"extensions/v1beta1\",\"enabled\":false,\"labels\":{},\"name\":null,\"spec\":{}},\"cloudName\":\"kubernetes\",\"componentName\":\"jenkins-controller\",\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true,\"runAsGroup\":1000,\"runAsUser\":1000},\"csrf\":{\"defaultCrumbIssuer\":{\"enabled\":true,\"proxyCompatability\":true}},\"customInitContainers\":[],\"customJenkinsLabels\":[],\"disableRememberMe\":false,\"disabledAgentProtocols\":[\"JNLP-connect\",\"JNLP2-connect\"],\"enableRawHtmlMarkupFormatter\":false,\"executorMode\":\"NORMAL\",\"existingSecret\":null,\"extraPorts\":[],\"fsGroup\":1000,\"googlePodMonitor\":{\"enabled\":false,\"scrapeEndpoint\":\"/prometheus\",\"scrapeInterval\":\"60s\"},\"healthProbes\":true,\"hostAliases\":[],\"hostNetworking\":false,\"httpsKeyStore\":{\"disableSecretMount\":false,\"enable\":false,\"fileName\":\"keystore.jks\",\"httpPort\":8081,\"jenkinsHttpsJksPasswordSecretKey\":\"https-jks-password\",\"jenkinsHttpsJksPasswordSecretName\":\"\",\"jenkinsHttpsJksSecretKey\":\"jenkins-jks-file\",\"jenkinsHttpsJksSecretName\":\"\",\"jenkinsKeyStoreBase64Encoded\":\"/u3+7QAAAAIAAAABAAAAAQANamVua2luc2NpLmNvbQAAAW2r/b1ZAAAFATCCBP0wDgYKKwYBBAEq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\\n\",\"password\":\"password\",\"path\":\"/var/jenkins_keystore\"},\"image\":\"jenkins/jenkins\",\"imagePullPolicy\":\"Always\",\"imagePullSecretName\":null,\"ingress\":{\"annotations\":{},\"apiVersion\":\"extensions/v1beta1\",\"enabled\":false,\"hostName\":null,\"labels\":{},\"paths\":[],\"tls\":null},\"initScripts\":[],\"initializeOnce\":false,\"installLatestPlugins\":true,\"installLatestSpecifiedPlugins\":false,\"installPlugins\":[\"kubernetes:3937.vd7b_82db_e347b_\",\"workflow-aggregator:596.v8c21c963d92d\",\"git:5.1.0\",\"configuration-as-code:1670.v564dc8b_982d0\"],\"jenkinsHome\":\"/var/jenkins_home\",\"jenkinsRef\":\"/usr/share/jenkins/ref\",\"jenkinsWar\":\"/usr/share/jenkins/jenkins.war\",\"lifecycle\":null,\"loadBalancerSourceRanges\":[\"0.0.0.0/0\"],\"markupFormatter\":\"plainText\",\"nodeSelector\":{},\"numExecutors\":0,\"overwritePluginsFromImage\":true,\"podAnnotations\":{},\"podDisruptionBudget\":{\"annotations\":{},\"apiVersion\":\"policy/v1beta1\",\"enabled\":false,\"labels\":{}},\"podLabels\":{},\"priorityClassName\":null,\"probes\":{\"livenessProbe\":{\"failureThreshold\":5,\"httpGet\":{\"path\":\"{{ default \\\"\\\" .Values.controller.jenkinsUriPrefix }}/login\",\"port\":\"http\"},\"periodSeconds\":10,\"timeoutSeconds\":5},\"readinessProbe\":{\"failureThreshold\":3,\"httpGet\":{\"path\":\"{{ default \\\"\\\" .Values.controller.jenkinsUriPrefix }}/login\",\"port\":\"http\"},\"periodSeconds\":10,\"timeoutSeconds\":5},\"startupProbe\":{\"failureThreshold\":12,\"httpGet\":{\"path\":\"{{ default \\\"\\\" .Values.controller.jenkinsUriPrefix }}/login\",\"port\":\"http\"},\"periodSeconds\":10,\"timeoutSeconds\":5}},\"projectNamingStrategy\":\"standard\",\"prometheus\":{\"alertingRulesAdditionalLabels\":{},\"alertingrules\":[],\"enabled\":false,\"metricRelabelings\":[],\"prometheusRuleNamespace\":\"\",\"relabelings\":[],\"scrapeEndpoint\":\"/prometheus\",\"scrapeInterval\":\"60s\",\"serviceMonitorAdditionalLabels\":{}},\"resources\":{\"limits\":{\"cpu\":\"2000m\",\"memory\":\"4096Mi\"},\"requests\":{\"cpu\":\"50m\",\"memory\":\"256Mi\"}},\"route\":{\"annotations\":{},\"enabled\":false,\"labels\":{}},\"runAsUser\":1000,\"schedulerName\":\"\",\"scriptApproval\":[],\"secondaryingress\":{\"annotations\":{},\"apiVersion\":\"extensions/v1beta1\",\"enabled\":false,\"hostName\":null,\"labels\":{},\"paths\":[],\"tls\":null},\"secretClaims\":[],\"securityContextCapabilities\":{},\"serviceAnnotations\":{},\"serviceExternalTrafficPolicy\":null,\"serviceLabels\":{},\"servicePort\":8080,\"serviceType\":\"ClusterIP\",\"shareProcessNamespace\":false,\"sidecars\":{\"configAutoReload\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"readOnlyRootFilesystem\":true},\"enabled\":true,\"folder\":\"/var/jenkins_home/casc_configs\",\"image\":\"kiwigrid/k8s-sidecar:1.24.4\",\"imagePullPolicy\":\"IfNotPresent\",\"reqRetryConnect\":10,\"resources\":{},\"sshTcpPort\":1044},\"other\":[]},\"statefulSetAnnotations\":{},\"statefulSetLabels\":{},\"tagLabel\":\"jdk11\",\"targetPort\":8080,\"terminationGracePeriodSeconds\":null,\"terminationMessagePath\":null,\"terminationMessagePolicy\":null,\"testEnabled\":true,\"tolerations\":[],\"updateStrategy\":{},\"usePodSecurityContext\":true},\"cronJob\":{\"apiVersion\":\"batch/v1\"},\"helmtest\":{\"bats\":{\"image\":\"bats/bats\",\"tag\":\"1.9.0\"}},\"kubernetesURL\":\"https://kubernetes.default\",\"networkPolicy\":{\"apiVersion\":\"networking.k8s.io/v1\",\"enabled\":false,\"externalAgents\":{},\"internalAgents\":{\"allowed\":true,\"namespaceLabels\":{},\"podLabels\":{}}},\"persistence\":{\"accessMode\":\"ReadWriteOnce\",\"annotations\":{},\"dataSource\":null,\"enabled\":true,\"existingClaim\":null,\"labels\":{},\"mounts\":null,\"size\":\"8Gi\",\"storageClass\":null,\"volumes\":null},\"rbac\":{\"create\":true,\"readSecrets\":false},\"renderHelmLabels\":true,\"serviceAccount\":{\"annotations\":{},\"create\":true,\"extraLabels\":{},\"imagePullSecretName\":null,\"name\":null},\"serviceAccountAgent\":{\"annotations\":{},\"create\":false,\"extraLabels\":{},\"imagePullSecretName\":null,\"name\":null}}",
                "version": "4.6.4"
              }
            ],
            "name": "jenkins",
            "namespace": "jenkins",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": null,
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "controller.adminPassword",
                "type": "",
                "value": "admin"
              },
              {
                "name": "controller.adminUser",
                "type": "",
                "value": "admin"
              }
            ],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Default values for jenkins.\n# This is a YAML-formatted file.\n# Declare name/value pairs to be passed into your templates.\n# name: value\n\n## Overrides for generated resource names\n# See templates/_helpers.tpl\n# nameOverride:\n# fullnameOverride:\n# namespaceOverride:\n\n# For FQDN resolving of the controller service. Change this value to match your existing configuration.\n# ref: https://github.com/kubernetes/dns/blob/master/docs/specification.md\nclusterZone: \"cluster.local\"\n\n# The URL of the Kubernetes API server\nkubernetesURL: \"https://kubernetes.default\"\n\nrenderHelmLabels: true\n\ncontroller:\n  # Used for label app.kubernetes.io/component\n  componentName: \"jenkins-controller\"\n  image: \"jenkins/jenkins\"\n  # tag: \"2.414.1-jdk11\"\n  tagLabel: jdk11\n  imagePullPolicy: \"Always\"\n  imagePullSecretName:\n  # Optionally configure lifetime for controller-container\n  lifecycle:\n  #  postStart:\n  #    exec:\n  #      command:\n  #      - \"uname\"\n  #      - \"-a\"\n  disableRememberMe: false\n  numExecutors: 0\n  # configures the executor mode of the Jenkins node. Possible values are: NORMAL or EXCLUSIVE\n  executorMode: \"NORMAL\"\n  # This is ignored if enableRawHtmlMarkupFormatter is true\n  markupFormatter: plainText\n  customJenkinsLabels: []\n  # The default configuration uses this secret to configure an admin user\n  # If you don't need that user or use a different security realm then you can disable it\n  adminSecret: true\n\n  hostNetworking: false\n  # When enabling LDAP or another non-Jenkins identity source, the built-in admin account will no longer exist.\n  # If you disable the non-Jenkins identity store and instead use the Jenkins internal one,\n  # you should revert controller.adminUser to your preferred admin user:\n  adminUser: \"admin\"\n  # adminPassword: \u003cdefaults to random\u003e\n  admin:\n    existingSecret: \"\"\n    userKey: jenkins-admin-user\n    passwordKey: jenkins-admin-password\n  # This values should not be changed unless you use your custom image of jenkins or any devired from. If you want to use\n  # Cloudbees Jenkins Distribution docker, you should set jenkinsHome: \"/var/cloudbees-jenkins-distribution\"\n  jenkinsHome: \"/var/jenkins_home\"\n  # This values should not be changed unless you use your custom image of jenkins or any devired from. If you want to use\n  # Cloudbees Jenkins Distribution docker, you should set jenkinsRef: \"/usr/share/cloudbees-jenkins-distribution/ref\"\n  jenkinsRef: \"/usr/share/jenkins/ref\"\n  # Path to the jenkins war file which is used by jenkins-plugin-cli.\n  jenkinsWar: \"/usr/share/jenkins/jenkins.war\"\n  # Overrides the default arguments passed to the war\n  # overrideArgs:\n  #   - --httpPort=8080\n  resources:\n    requests:\n      cpu: \"50m\"\n      memory: \"256Mi\"\n    limits:\n      cpu: \"2000m\"\n      memory: \"4096Mi\"\n  # Share process namespace to allow sidecar containers to interact with processes in other containers in the same pod\n  shareProcessNamespace: false\n  # Overrides the init container default values\n  # initContainerResources:\n  #   requests:\n  #     cpu: \"50m\"\n  #     memory: \"256Mi\"\n  #   limits:\n  #     cpu: \"2000m\"\n  #     memory: \"4096Mi\"\n  # Environment variables that get added to the init container (useful for e.g. http_proxy)\n  # initContainerEnv:\n  #   - name: http_proxy\n  #     value: \"http://192.168.64.1:3128\"\n  # containerEnv:\n  #   - name: http_proxy\n  #     value: \"http://192.168.64.1:3128\"\n  # Set min/max heap here if needed with:\n  # javaOpts: \"-Xms512m -Xmx512m\"\n  # jenkinsOpts: \"\"\n  # If you are using the ingress definitions provided by this chart via the `controller.ingress` block the configured hostname will be the ingress hostname starting with `https://` or `http://` depending on the `tls` configuration.\n  # The Protocol can be overwritten by specifying `controller.jenkinsUrlProtocol`.\n  # jenkinsUrlProtocol: \"https\"\n  # If you are not using the provided ingress you can specify `controller.jenkinsUrl` to change the url definition.\n  # jenkinsUrl: \"\"\n  # If you set this prefix and use ingress controller then you might want to set the ingress path below\n  # jenkinsUriPrefix: \"/jenkins\"\n  # Enable pod security context (must be `true` if podSecurityContextOverride, runAsUser or fsGroup are set)\n  usePodSecurityContext: true\n  # Note that `runAsUser`, `fsGroup`, and `securityContextCapabilities` are\n  # being deprecated and replaced by `podSecurityContextOverride`.\n  # Set runAsUser to 1000 to let Jenkins run as non-root user 'jenkins' which exists in 'jenkins/jenkins' docker image.\n  # When setting runAsUser to a different value than 0 also set fsGroup to the same value:\n  runAsUser: 1000\n  fsGroup: 1000\n  # If you have PodSecurityPolicies that require dropping of capabilities as suggested by CIS K8s benchmark, put them here\n  securityContextCapabilities: {}\n  #  drop:\n  #    - NET_RAW\n  # Completely overwrites the contents of the `securityContext`, ignoring the\n  # values provided for the deprecated fields: `runAsUser`, `fsGroup`, and\n  # `securityContextCapabilities`.  In the case of mounting an ext4 filesystem,\n  # it might be desirable to use `supplementalGroups` instead of `fsGroup` in\n  # the `securityContext` block: https://github.com/kubernetes/kubernetes/issues/67014#issuecomment-589915496\n  # podSecurityContextOverride:\n  #   runAsUser: 1000\n  #   runAsNonRoot: true\n  #   supplementalGroups: [1000]\n  #   # capabilities: {}\n  # Container securityContext\n  containerSecurityContext:\n    runAsUser: 1000\n    runAsGroup: 1000\n    readOnlyRootFilesystem: true\n    allowPrivilegeEscalation: false\n  servicePort: 8080\n  targetPort: 8080\n  # For minikube, set this to NodePort, elsewhere use LoadBalancer\n  # Use ClusterIP if your setup includes ingress controller\n  serviceType: ClusterIP\n  # Use Local to preserve the client source IP and avoids a second hop for LoadBalancer and Nodeport type services,\n  # but risks potentially imbalanced traffic spreading.\n  serviceExternalTrafficPolicy:\n  # Jenkins controller service annotations\n  serviceAnnotations: {}\n  # Jenkins controller custom labels\n  statefulSetLabels: {}\n  #   foo: bar\n  #   bar: foo\n  # Jenkins controller service labels\n  serviceLabels: {}\n  #   service.beta.kubernetes.io/aws-load-balancer-backend-protocol: https\n  # Put labels on Jenkins controller pod\n  podLabels: {}\n  # Used to create Ingress record (should be used with ServiceType: ClusterIP)\n  # nodePort: \u003cto set explicitly, choose port between 30000-32767\n  # Enable Kubernetes Startup, Liveness and Readiness Probes\n  # if Startup Probe is supported, enable it too\n  # ~ 2 minutes to allow Jenkins to restart when upgrading plugins. Set ReadinessTimeout to be shorter than LivenessTimeout.\n  healthProbes: true\n  probes:\n    startupProbe:\n      httpGet:\n        path: '{{ default \"\" .Values.controller.jenkinsUriPrefix }}/login'\n        port: http\n      periodSeconds: 10\n      timeoutSeconds: 5\n      failureThreshold: 12\n    livenessProbe:\n      failureThreshold: 5\n      httpGet:\n        path: '{{ default \"\" .Values.controller.jenkinsUriPrefix }}/login'\n        port: http\n      periodSeconds: 10\n      timeoutSeconds: 5\n      # If Startup Probe is not supported on your Kubernetes cluster, you might want to use \"initialDelaySeconds\" instead.\n      # It delays the initial liveness probe while Jenkins is starting\n      # initialDelaySeconds: 60\n    readinessProbe:\n      failureThreshold: 3\n      httpGet:\n        path: '{{ default \"\" .Values.controller.jenkinsUriPrefix }}/login'\n        port: http\n      periodSeconds: 10\n      timeoutSeconds: 5\n      # If Startup Probe is not supported on your Kubernetes cluster, you might want to use \"initialDelaySeconds\" instead.\n      # It delays the initial readyness probe while Jenkins is starting\n      # initialDelaySeconds: 60\n\n  # PodDisruptionBudget config\n  podDisruptionBudget:\n    enabled: false\n    # For Kubernetes v1.5+, use 'policy/v1beta1'\n    # For Kubernetes v1.21+, use 'policy/v1'\n    apiVersion: \"policy/v1beta1\"\n    annotations: {}\n    labels: {}\n    # maxUnavailable: \"0\"\n\n  agentListenerEnabled: true\n  agentListenerPort: 50000\n  agentListenerHostPort:\n  agentListenerNodePort:\n  agentListenerExternalTrafficPolicy:\n  agentListenerLoadBalancerSourceRanges:\n  - 0.0.0.0/0\n  disabledAgentProtocols:\n    - JNLP-connect\n    - JNLP2-connect\n  csrf:\n    defaultCrumbIssuer:\n      enabled: true\n      proxyCompatability: true\n  # Kubernetes service type for the JNLP agent service\n  # agentListenerServiceType is the Kubernetes Service type for the JNLP agent service,\n  # either 'LoadBalancer', 'NodePort', or 'ClusterIP'\n  # Note if you set this to 'LoadBalancer', you *must* define annotations to secure it. By default\n  # this will be an external load balancer and allowing inbound 0.0.0.0/0, a HUGE\n  # security risk:  https://github.com/kubernetes/charts/issues/1341\n  agentListenerServiceType: \"ClusterIP\"\n  # Optionally assign an IP to the LoadBalancer agentListenerService LoadBalancer\n  # GKE users: only regional static IPs will work for Service Load balancer.\n  agentListenerLoadBalancerIP:\n  agentListenerServiceAnnotations: {}\n\n  # Example of 'LoadBalancer' type of agent listener with annotations securing it\n  # agentListenerServiceType: LoadBalancer\n  # agentListenerServiceAnnotations:\n  #   service.beta.kubernetes.io/aws-load-balancer-internal: \"True\"\n  #   service.beta.kubernetes.io/load-balancer-source-ranges: \"172.0.0.0/8, 10.0.0.0/8\"\n\n  # LoadBalancerSourcesRange is a list of allowed CIDR values, which are combined with ServicePort to\n  # set allowed inbound rules on the security group assigned to the controller load balancer\n  loadBalancerSourceRanges:\n  - 0.0.0.0/0\n  # Optionally assign a known public LB IP\n  # loadBalancerIP: 1.2.3.4\n  # Optionally configure a JMX port\n  # requires additional javaOpts, ie\n  # javaOpts: \u003e\n  #   -Dcom.sun.management.jmxremote.port=4000\n  #   -Dcom.sun.management.jmxremote.authenticate=false\n  #   -Dcom.sun.management.jmxremote.ssl=false\n  # jmxPort: 4000\n  # Optionally configure other ports to expose in the controller container\n  extraPorts: []\n  # - name: BuildInfoProxy\n  #   port: 9000\n  #   targetPort: 9010 (Optional: Use to explicitly set targetPort if different from port)\n\n  # List of plugins to be install during Jenkins controller start\n  installPlugins:\n    - kubernetes:3937.vd7b_82db_e347b_\n    - workflow-aggregator:596.v8c21c963d92d\n    - git:5.1.0\n    - configuration-as-code:1670.v564dc8b_982d0\n\n  # Set to false to download the minimum required version of all dependencies.\n  installLatestPlugins: true\n\n  # Set to true to download latest dependencies of any plugin that is requested to have the latest version.\n  installLatestSpecifiedPlugins: false\n\n  # List of plugins to install in addition to those listed in controller.installPlugins\n  additionalPlugins: []\n\n  # Enable to initialize the Jenkins controller only once on initial installation.\n  # Without this, whenever the controller gets restarted (Evicted, etc.) it will fetch plugin updates which has the potential to cause breakage.\n  # Note that for this to work, `persistence.enabled` needs to be set to `true`\n  initializeOnce: false\n\n  # Enable to always override the installed plugins with the values of 'controller.installPlugins' on upgrade or redeployment.\n  # overwritePlugins: true\n\n  # Configures if plugins bundled with `controller.image` should be overwritten with the values of 'controller.installPlugins' on upgrade or redeployment.\n  overwritePluginsFromImage: true\n\n  # Configures the restrictions for naming projects. Set this key to null or empty to skip it in the default config.\n  projectNamingStrategy: standard\n\n  # Enable HTML parsing using OWASP Markup Formatter Plugin (antisamy-markup-formatter), useful with ghprb plugin.\n  # The plugin is not installed by default, please update controller.installPlugins.\n  enableRawHtmlMarkupFormatter: false\n  # Used to approve a list of groovy functions in pipelines used the script-security plugin. Can be viewed under /scriptApproval\n  scriptApproval: []\n  #  - \"method groovy.json.JsonSlurperClassic parseText java.lang.String\"\n  #  - \"new groovy.json.JsonSlurperClassic\"\n  # List of groovy init scripts to be executed during Jenkins controller start\n  initScripts: []\n  #  - |\n  #    print 'adding global pipeline libraries, register properties, bootstrap jobs...'\n\n  # 'name' is a name of an existing secret in same namespace as jenkins,\n  # 'keyName' is the name of one of the keys inside current secret.\n  # the 'name' and 'keyName' are concatenated with a '-' in between, so for example:\n  # an existing secret \"secret-credentials\" and a key inside it named \"github-password\" should be used in Jcasc as ${secret-credentials-github-password}\n  # 'name' and 'keyName' must be lowercase RFC 1123 label must consist of lower case alphanumeric characters or '-',\n  # and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc')\n  # existingSecret existing secret \"secret-credentials\" and a key inside it named \"github-username\" should be used in Jcasc as ${github-username}\n  # When using existingSecret no need to specify the keyName under additionalExistingSecrets.\n  existingSecret:\n\n  additionalExistingSecrets: []\n  #  - name: secret-name-1\n  #    keyName: username\n  #  - name: secret-name-1\n  #    keyName: password\n\n  additionalSecrets: []\n  #  - name: nameOfSecret\n  #    value: secretText\n\n  # Generate SecretClaim resources in order to create Kubernetes secrets from HashiCorp Vault using kube-vault-controller.\n  # 'name' is name of the secret that will be created in Kubernetes. The Jenkins fullname is prepended to this value.\n  # 'path' is the fully qualified path to the secret in Vault\n  # 'type' is an optional Kubernetes secret type. Defaults to 'Opaque'\n  # 'renew' is an optional secret renewal time in seconds\n  secretClaims: []\n  # - name: secretName        # required\n  #   path: testPath          # required\n  #   type: kubernetes.io/tls # optional\n  #   renew: 60               # optional\n\n  # Name of default cloud configuration.\n  cloudName: \"kubernetes\"\n\n  # Below is the implementation of Jenkins Configuration as Code.  Add a key under configScripts for each configuration area,\n  # where each corresponds to a plugin or section of the UI.  Each key (prior to | character) is just a label, and can be any value.\n  # Keys are only used to give the section a meaningful name.  The only restriction is they may only contain RFC 1123 \\ DNS label\n  # characters: lowercase letters, numbers, and hyphens.  The keys become the name of a configuration yaml file on the controller in\n  # /var/jenkins_home/casc_configs (by default) and will be processed by the Configuration as Code Plugin.  The lines after each |\n  # become the content of the configuration yaml file.  The first line after this is a JCasC root element, eg jenkins, credentials,\n  # etc.  Best reference is https://\u003cjenkins_url\u003e/configuration-as-code/reference.  The example below creates a welcome message:\n  JCasC:\n    defaultConfig: true\n    configUrls: []\n    # - https://acme.org/jenkins.yaml\n    # Remote URL:s for configuration files.\n    configScripts: {}\n    #  welcome-message: |\n    #    jenkins:\n    #      systemMessage: Welcome to our CI\\CD server.  This Jenkins is configured and managed 'as code'.\n    # Allows adding to the top-level security JCasC section. For legacy,  default the chart includes apiToken configurations\n    security:\n      apiToken:\n        creationOfLegacyTokenEnabled: false\n        tokenGenerationOnCreationEnabled: false\n        usageStatisticsEnabled: true\n    # Ignored if securityRealm is defined in controller.JCasC.configScripts\n    securityRealm: |-\n      local:\n        allowsSignup: false\n        enableCaptcha: false\n        users:\n        - id: \"${chart-admin-username}\"\n          name: \"Jenkins Admin\"\n          password: \"${chart-admin-password}\"\n    # Ignored if authorizationStrategy is defined in controller.JCasC.configScripts\n    authorizationStrategy: |-\n      loggedInUsersCanDoAnything:\n        allowAnonymousRead: false\n  # Optionally specify additional init-containers\n  customInitContainers: []\n  # - name: custom-init\n  #   image: \"alpine:3.7\"\n  #   imagePullPolicy: Always\n  #   command: [ \"uname\", \"-a\" ]\n\n  sidecars:\n    configAutoReload:\n      # If enabled: true, Jenkins Configuration as Code will be reloaded on-the-fly without a reboot.  If false or not-specified,\n      # jcasc changes will cause a reboot and will only be applied at the subsequent start-up.  Auto-reload uses the\n      # http://\u003cjenkins_url\u003e/reload-configuration-as-code endpoint to reapply config when changes to the configScripts are detected.\n      enabled: true\n      image: kiwigrid/k8s-sidecar:1.24.4\n      imagePullPolicy: IfNotPresent\n      resources: {}\n        #   limits:\n        #     cpu: 100m\n        #     memory: 100Mi\n        #   requests:\n        #     cpu: 50m\n        #     memory: 50Mi\n      # How many connection-related errors to retry on\n      reqRetryConnect: 10\n      # env:\n      #   - name: REQ_TIMEOUT\n      #     value: \"30\"\n      # SSH port value can be set to any unused TCP port.  The default, 1044, is a non-standard SSH port that has been chosen at random.\n      # Is only used to reload jcasc config from the sidecar container running in the Jenkins controller pod.\n      # This TCP port will not be open in the pod (unless you specifically configure this), so Jenkins will not be\n      # accessible via SSH from outside of the pod.  Note if you use non-root pod privileges (runAsUser \u0026 fsGroup),\n      # this must be \u003e 1024:\n      sshTcpPort: 1044\n      # folder in the pod that should hold the collected dashboards:\n      folder: \"/var/jenkins_home/casc_configs\"\n      # If specified, the sidecar will search for JCasC config-maps inside this namespace.\n      # Otherwise the namespace in which the sidecar is running will be used.\n      # It's also possible to specify ALL to search in all namespaces:\n      # searchNamespace:\n      containerSecurityContext:\n        readOnlyRootFilesystem: true\n        allowPrivilegeEscalation: false\n\n    # Allows you to inject additional/other sidecars\n    other: []\n    ## The example below runs the client for https://smee.io as sidecar container next to Jenkins,\n    ## that allows to trigger build behind a secure firewall.\n    ## https://jenkins.io/blog/2019/01/07/webhook-firewalls/#triggering-builds-with-webhooks-behind-a-secure-firewall\n    ##\n    ## Note: To use it you should go to https://smee.io/new and update the url to the generete one.\n    # - name: smee\n    #   image: docker.io/twalter/smee-client:1.0.2\n    #   args: [\"--port\", \"{{ .Values.controller.servicePort }}\", \"--path\", \"/github-webhook/\", \"--url\", \"https://smee.io/new\"]\n    #   resources:\n    #     limits:\n    #       cpu: 50m\n    #       memory: 128Mi\n    #     requests:\n    #       cpu: 10m\n    #       memory: 32Mi\n  # Name of the Kubernetes scheduler to use\n  schedulerName: \"\"\n  # Node labels and tolerations for pod assignment\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector\n  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#taints-and-tolerations-beta-feature\n  nodeSelector: {}\n\n  terminationGracePeriodSeconds:\n\n  terminationMessagePath:\n  terminationMessagePolicy:\n\n  tolerations: []\n\n  affinity: {}\n  # Leverage a priorityClass to ensure your pods survive resource shortages\n  # ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n  priorityClassName:\n\n  podAnnotations: {}\n  # Add StatefulSet annotations\n  statefulSetAnnotations: {}\n\n  # StatefulSet updateStrategy\n  # ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies\n  updateStrategy: {}\n\n  ingress:\n    enabled: false\n    # Override for the default paths that map requests to the backend\n    paths: []\n    # - backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n    # - backend:\n    #     serviceName: \u003e-\n    #       {{ template \"jenkins.fullname\" . }}\n    #     # Don't use string here, use only integer value!\n    #     servicePort: 8080\n    # For Kubernetes v1.14+, use 'networking.k8s.io/v1beta1'\n    # For Kubernetes v1.19+, use 'networking.k8s.io/v1'\n    apiVersion: \"extensions/v1beta1\"\n    labels: {}\n    annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n    # Set this path to jenkinsUriPrefix above or use annotations to rewrite path\n    # path: \"/jenkins\"\n    # configures the hostname e.g. jenkins.example.com\n    hostName:\n    tls:\n    # - secretName: jenkins.cluster.local\n    #   hosts:\n    #     - jenkins.cluster.local\n\n  # often you want to have your controller all locked down and private\n  # but you still want to get webhooks from your SCM\n  # A secondary ingress will let you expose different urls\n  # with a differnt configuration\n  secondaryingress:\n    enabled: false\n    # paths you want forwarded to the backend\n    # ex /github-webhook\n    paths: []\n    # For Kubernetes v1.14+, use 'networking.k8s.io/v1beta1'\n    # For Kubernetes v1.19+, use 'networking.k8s.io/v1'\n    apiVersion: \"extensions/v1beta1\"\n    labels: {}\n    annotations: {}\n    # kubernetes.io/ingress.class: nginx\n    # kubernetes.io/tls-acme: \"true\"\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n    # configures the hostname e.g. jenkins-external.example.com\n    hostName:\n    tls:\n    # - secretName: jenkins-external.example.com\n    #   hosts:\n    #     - jenkins-external.example.com\n\n  # If you're running on GKE and need to configure a backendconfig\n  # to finish ingress setup, use the following values.\n  # Docs: https://cloud.google.com/kubernetes-engine/docs/concepts/backendconfig\n  backendconfig:\n    enabled: false\n    apiVersion: \"extensions/v1beta1\"\n    name:\n    labels: {}\n    annotations: {}\n    spec: {}\n\n  # Openshift route\n  route:\n    enabled: false\n    labels: {}\n    annotations: {}\n    # path: \"/jenkins\"\n\n  # controller.hostAliases allows for adding entries to Pod /etc/hosts:\n  # https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/\n  hostAliases: []\n  # - ip: 192.168.50.50\n  #   hostnames:\n  #     - something.local\n  # - ip: 10.0.50.50\n  #   hostnames:\n  #     - other.local\n\n  # Expose Prometheus metrics\n  prometheus:\n    # If enabled, add the prometheus plugin to the list of plugins to install\n    # https://plugins.jenkins.io/prometheus\n    enabled: false\n    # Additional labels to add to the ServiceMonitor object\n    serviceMonitorAdditionalLabels: {}\n    # Set a custom namespace where to deploy ServiceMonitor resource\n    # serviceMonitorNamespace: monitoring\n    scrapeInterval: 60s\n    # This is the default endpoint used by the prometheus plugin\n    scrapeEndpoint: /prometheus\n    # Additional labels to add to the PrometheusRule object\n    alertingRulesAdditionalLabels: {}\n    # An array of prometheus alerting rules\n    # See here: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n    # The `groups` root object is added by default, simply add the rule entries\n    alertingrules: []\n    # Set a custom namespace where to deploy PrometheusRule resource\n    prometheusRuleNamespace: \"\"\n\n    # RelabelConfigs to apply to samples before scraping. Prometheus Operator automatically adds\n    # relabelings for a few standard Kubernetes fields. The original scrape job’s name\n    # is available via the __tmp_prometheus_job_name label.\n    # More info: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config\n    relabelings: []\n    # MetricRelabelConfigs to apply to samples before ingestion.\n    metricRelabelings: []\n\n  googlePodMonitor:\n    # If enabled, It creates Google Managed Prometheus scraping config\n    enabled: false\n    # Set a custom namespace where to deploy PodMonitoring resource\n    # serviceMonitorNamespace: \"\"\n    scrapeInterval: 60s\n    # This is the default endpoint used by the prometheus plugin\n    scrapeEndpoint: /prometheus\n\n  # Can be used to disable rendering controller test resources when using helm template\n  testEnabled: true\n\n  httpsKeyStore:\n    jenkinsHttpsJksSecretName: ''\n    jenkinsHttpsJksSecretKey: \"jenkins-jks-file\"\n    jenkinsHttpsJksPasswordSecretName: \"\"\n    jenkinsHttpsJksPasswordSecretKey: \"https-jks-password\"\n    enable: false\n    disableSecretMount: false\n    httpPort: 8081\n    path: \"/var/jenkins_keystore\"\n    fileName: \"keystore.jks\"\n    password: \"password\"\n    # Convert keystore.jks files content to base64 ( cat keystore.jks | base64 ) and put the output here\n    jenkinsKeyStoreBase64Encoded: |\n        /u3+7QAAAAIAAAABAAAAAQANamVua2luc2NpLmNvbQAAAW2r/b1ZAAAFATCCBP0wDgYKKwYBBAEq\n        AhEBAQUABIIE6QbCqasvoHS0pSwYqSvdydMCB9t+VNfwhFIiiuAelJfO5sSe2SebJbtwHgLcRz1Z\n        gMtWgOSFdl3bWSzA7vrW2LED52h+jXLYSWvZzuDuh8hYO85m10ikF6QR+dTi4jra0whIFDvq3pxe\n        TnESxEsN+DvbZM3jA3qsjQJSeISNpDjO099dqQvHpnCn18lyk7J4TWJ8sOQQb1EM2zDAfAOSqA/x\n        QuPEFl74DlY+5DIk6EBvpmWhaMSvXzWZACGA0sYqa157dq7O0AqmuLG/EI5EkHETO4CrtBW+yLcy\n        2dUCXOMA+j+NjM1BjrQkYE5vtSfNO6lFZcISyKo5pTFlcA7ut0Fx2nZ8GhHTn32CpeWwNcZBn1gR\n        pZVt6DxVVkhTAkMLhR4rL2wGIi/1WRs23ZOLGKtyDNvDHnQyDiQEoJGy9nAthA8aNHa3cfdF10vB\n        Drb19vtpFHmpvKEEhpk2EBRF4fTi644Fuhu2Ied6118AlaPvEea+n6G4vBz+8RWuVCmZjLU+7h8l\n        Hy3/WdUPoIL5eW7Kz+hS+sRTFzfu9C48dMkQH3a6f3wSY+mufizNF9U298r98TnYy+PfDJK0bstG\n        Ph6yPWx8DGXKQBwrhWJWXI6JwZDeC5Ny+l8p1SypTmAjpIaSW3ge+KgcL6Wtt1R5hUV1ajVwVSUi\n        HF/FachKqPqyLJFZTGjNrxnmNYpt8P1d5JTvJfmfr55Su/P9n7kcyWp7zMcb2Q5nlXt4tWogOHLI\n        OzEWKCacbFfVHE+PpdrcvCVZMDzFogIq5EqGTOZe2poPpBVE+1y9mf5+TXBegy5HToLWvmfmJNTO\n        NCDuBjgLs2tdw2yMPm4YEr57PnMX5gGTC3f2ZihXCIJDCRCdQ9sVBOjIQbOCzxFXkVITo0BAZhCi\n        Yz61wt3Ud8e//zhXWCkCsSV+IZCxxPzhEFd+RFVjW0Nm9hsb2FgAhkXCjsGROgoleYgaZJWvQaAg\n        UyBzMmKDPKTllBHyE3Gy1ehBNGPgEBChf17/9M+j8pcm1OmlM434ctWQ4qW7RU56//yq1soFY0Te\n        fu2ei03a6m68fYuW6s7XEEK58QisJWRAvEbpwu/eyqfs7PsQ+zSgJHyk2rO95IxdMtEESb2GRuoi\n        Bs+AHNdYFTAi+GBWw9dvEgqQ0Mpv0//6bBE/Fb4d7b7f56uUNnnE7mFnjGmGQN+MvC62pfwfvJTT\n        EkT1iZ9kjM9FprTFWXT4UmO3XTvesGeE50sV9YPm71X4DCQwc4KE8vyuwj0s6oMNAUACW2ClU9QQ\n        y0tRpaF1tzs4N42Q5zl0TzWxbCCjAtC3u6xf+c8MCGrr7DzNhm42LOQiHTa4MwX4x96q7235oiAU\n        iQqSI/hyF5yLpWw4etyUvsx2/0/0wkuTU1FozbLoCWJEWcPS7QadMrRRISxHf0YobIeQyz34regl\n        t1qSQ3dCU9D6AHLgX6kqllx4X0fnFq7LtfN7fA2itW26v+kAT2QFZ3qZhINGfofCja/pITC1uNAZ\n        gsJaTMcQ600krj/ynoxnjT+n1gmeqThac6/Mi3YlVeRtaxI2InL82ZuD+w/dfY9OpPssQjy3xiQa\n        jPuaMWXRxz/sS9syOoGVH7XBwKrWpQcpchozWJt40QV5DslJkclcr8aC2AGlzuJMTdEgz1eqV0+H\n        bAXG9HRHN/0eJTn1/QAAAAEABVguNTA5AAADjzCCA4swggJzAhRGqVxH4HTLYPGO4rzHcCPeGDKn\n        xTANBgkqhkiG9w0BAQsFADCBgTELMAkGA1UEBhMCY2ExEDAOBgNVBAgMB29udGFyaW8xEDAOBgNV\n        BAcMB3Rvcm9udG8xFDASBgNVBAoMC2plbmtpbnN0ZXN0MRkwFwYDVQQDDBBqZW5raW5zdGVzdC5p\n        bmZvMR0wGwYJKoZIhvcNAQkBFg50ZXN0QHRlc3QuaW5mbzAeFw0xOTEwMDgxNTI5NTVaFw0xOTEx\n        MDcxNTI5NTVaMIGBMQswCQYDVQQGEwJjYTEQMA4GA1UECAwHb250YXJpbzEQMA4GA1UEBwwHdG9y\n        b250bzEUMBIGA1UECgwLamVua2luc3Rlc3QxGTAXBgNVBAMMEGplbmtpbnN0ZXN0LmluZm8xHTAb\n        BgkqhkiG9w0BCQEWDnRlc3RAdGVzdC5pbmZvMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKC\n        AQEA02q352JTHGvROMBhSHvSv+vnoOTDKSTz2aLQn0tYrIRqRo+8bfmMjXuhkwZPSnCpvUGNAJ+w\n        Jrt/dqMoYUjCBkjylD/qHmnXN5EwS1cMg1Djh65gi5JJLFJ7eNcoSsr/0AJ+TweIal1jJSP3t3PF\n        9Uv21gm6xdm7HnNK66WpUUXLDTKaIs/jtagVY1bLOo9oEVeLN4nT2CYWztpMvdCyEDUzgEdDbmrP\n        F5nKUPK5hrFqo1Dc5rUI4ZshL3Lpv398aMxv6n2adQvuL++URMEbXXBhxOrT6rCtYzbcR5fkwS9i\n        d3Br45CoWOQro02JAepoU0MQKY5+xQ4Bq9Q7tB9BAwIDAQABMA0GCSqGSIb3DQEBCwUAA4IBAQAe\n        4xc+mSvKkrKBHg9/zpkWgZUiOp4ENJCi8H4tea/PCM439v6y/kfjT/okOokFvX8N5aa1OSz2Vsrl\n        m8kjIc6hiA7bKzT6lb0EyjUShFFZ5jmGVP4S7/hviDvgB5yEQxOPpumkdRP513YnEGj/o9Pazi5h\n        /MwpRxxazoda9r45kqQpyG+XoM4pB+Fd3JzMc4FUGxfVPxJU4jLawnJJiZ3vqiSyaB0YyUL+Er1Q\n        6NnqtR4gEBF0ZVlQmkycFvD4EC2boP943dLqNUvop+4R3SM1QMM6P5u8iTXtHd/VN4MwMyy1wtog\n        hYAzODo1Jt59pcqqKJEas0C/lFJEB3frw4ImNx5fNlJYOpx+ijfQs9m39CevDq0=\n\nagent:\n  enabled: true\n  defaultsProviderTemplate: \"\"\n  # URL for connecting to the Jenkins controller\n  jenkinsUrl:\n  # connect to the specified host and port, instead of connecting directly to the Jenkins controller\n  jenkinsTunnel:\n  kubernetesConnectTimeout: 5\n  kubernetesReadTimeout: 15\n  maxRequestsPerHostStr: \"32\"\n  namespace:\n  # private registry for agent image\n  jnlpregistry:\n  image: \"jenkins/inbound-agent\"\n  tag: \"3107.v665000b_51092-15\"\n  workingDir: \"/home/jenkins/agent\"\n  nodeUsageMode: \"NORMAL\"\n  customJenkinsLabels: []\n  # name of the secret to be used for image pulling\n  imagePullSecretName:\n  componentName: \"jenkins-agent\"\n  websocket: false\n  directConnection: false\n  privileged: false\n  runAsUser:\n  runAsGroup:\n  hostNetworking: false\n  resources:\n    requests:\n      cpu: \"512m\"\n      memory: \"512Mi\"\n    limits:\n      cpu: \"512m\"\n      memory: \"512Mi\"\n  livenessProbe: {}\n#    execArgs: \"cat /tmp/healthy\"\n#    failureThreshold: 3\n#    initialDelaySeconds: 0\n#    periodSeconds: 10\n#    successThreshold: 1\n#    timeoutSeconds: 1\n  # You may want to change this to true while testing a new image\n  alwaysPullImage: false\n  # Controls how agent pods are retained after the Jenkins build completes\n  # Possible values: Always, Never, OnFailure\n  podRetention: \"Never\"\n  # Disable if you do not want the Yaml the agent pod template to show up\n  # in the job Console Output. This can be helpful for either security reasons\n  # or simply to clean up the output to make it easier to read.\n  showRawYaml: true\n  # You can define the volumes that you want to mount for this container\n  # Allowed types are: ConfigMap, EmptyDir, HostPath, Nfs, PVC, Secret\n  # Configure the attributes as they appear in the corresponding Java class for that type\n  # https://github.com/jenkinsci/kubernetes-plugin/tree/master/src/main/java/org/csanchez/jenkins/plugins/kubernetes/volumes\n  volumes: []\n  # - type: ConfigMap\n  #   configMapName: myconfigmap\n  #   mountPath: /var/myapp/myconfigmap\n  # - type: EmptyDir\n  #   mountPath: /var/myapp/myemptydir\n  #   memory: false\n  # - type: HostPath\n  #   hostPath: /var/lib/containers\n  #   mountPath: /var/myapp/myhostpath\n  # - type: Nfs\n  #   mountPath: /var/myapp/mynfs\n  #   readOnly: false\n  #   serverAddress: \"192.0.2.0\"\n  #   serverPath: /var/lib/containers\n  # - type: PVC\n  #   claimName: mypvc\n  #   mountPath: /var/myapp/mypvc\n  #   readOnly: false\n  # - type: Secret\n  #   defaultMode: \"600\"\n  #   mountPath: /var/myapp/mysecret\n  #   secretName: mysecret\n  # Pod-wide environment, these vars are visible to any container in the agent pod\n\n  # You can define the workspaceVolume that you want to mount for this container\n  # Allowed types are: DynamicPVC, EmptyDir, HostPath, Nfs, PVC\n  # Configure the attributes as they appear in the corresponding Java class for that type\n  # https://github.com/jenkinsci/kubernetes-plugin/tree/master/src/main/java/org/csanchez/jenkins/plugins/kubernetes/volumes/workspace\n  workspaceVolume: {}\n  ## DynamicPVC example\n  # type: DynamicPVC\n  # configMapName: myconfigmap\n  ## EmptyDir example\n  # type: EmptyDir\n  # memory: false\n  ## HostPath example\n  # type: HostPath\n  # hostPath: /var/lib/containers\n  ## NFS example\n  # type: Nfs\n  # readOnly: false\n  # serverAddress: \"192.0.2.0\"\n  # serverPath: /var/lib/containers\n  ## PVC example\n  # type: PVC\n  # claimName: mypvc\n  # readOnly: false\n  #\n  # Pod-wide environment, these vars are visible to any container in the agent pod\n  envVars: []\n  # - name: PATH\n  #   value: /usr/local/bin\n  # Mount a secret as environment variable\n  secretEnvVars: []\n  # - key: PATH\n  #   optional: false # default: false\n  #   secretKey: MY-K8S-PATH\n  #   secretName: my-k8s-secret\n  nodeSelector: {}\n  # Key Value selectors. Ex:\n  # jenkins-agent: v1\n\n  # Executed command when side container gets started\n  command:\n  args: \"${computer.jnlpmac} ${computer.name}\"\n  # Side container name\n  sideContainerName: \"jnlp\"\n  # Doesn't allocate pseudo TTY by default\n  TTYEnabled: false\n  # Max number of spawned agent\n  containerCap: 10\n  # Pod name\n  podName: \"default\"\n  # Allows the Pod to remain active for reuse until the configured number of\n  # minutes has passed since the last step was executed on it.\n  idleMinutes: 0\n  # Raw yaml template for the Pod. For example this allows usage of toleration for agent pods.\n  # https://github.com/jenkinsci/kubernetes-plugin#using-yaml-to-define-pod-templates\n  # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  yamlTemplate: \"\"\n  # yamlTemplate: |-\n  #   apiVersion: v1\n  #   kind: Pod\n  #   spec:\n  #     tolerations:\n  #     - key: \"key\"\n  #       operator: \"Equal\"\n  #       value: \"value\"\n  # Defines how the raw yaml field gets merged with yaml definitions from inherited pod templates: merge or override\n  yamlMergeStrategy: \"override\"\n  # Timeout in seconds for an agent to be online\n  connectTimeout: 100\n  # Annotations to apply to the pod.\n  annotations: {}\n\n  # Add additional containers to the agents.\n  # Containers specified here are added to all agents. Set key empty to remove container from additional agents.\n  additionalContainers: []\n  #  - sideContainerName: dind\n  #    image: docker\n  #    tag: dind\n  #    command: dockerd-entrypoint.sh\n  #    args: \"\"\n  #    privileged: true\n  #    resources:\n  #      requests:\n  #        cpu: 500m\n  #        memory: 1Gi\n  #      limits:\n  #        cpu: 1\n  #        memory: 2Gi\n\n  # Disable the default Jenkins Agent configuration.\n  # Useful when configuring agents only with the podTemplates value, since the default podTemplate populated by values mentioned above will be excluded in the rendered template.\n  disableDefaultAgent: false\n\n  # Below is the implementation of custom pod templates for the default configured kubernetes cloud.\n  # Add a key under podTemplates for each pod template. Each key (prior to | character) is just a label, and can be any value.\n  # Keys are only used to give the pod template a meaningful name.  The only restriction is they may only contain RFC 1123 \\ DNS label\n  # characters: lowercase letters, numbers, and hyphens. Each pod template can contain multiple containers.\n  # For this pod templates configuration to be loaded the following values must be set:\n  # controller.JCasC.defaultConfig: true\n  # Best reference is https://\u003cjenkins_url\u003e/configuration-as-code/reference#Cloud-kubernetes. The example below creates a python pod template.\n  podTemplates: {}\n  #  python: |\n  #    - name: python\n  #      label: jenkins-python\n  #      serviceAccount: jenkins\n  #      containers:\n  #        - name: python\n  #          image: python:3\n  #          command: \"/bin/sh -c\"\n  #          args: \"cat\"\n  #          ttyEnabled: true\n  #          privileged: true\n  #          resourceRequestCpu: \"400m\"\n  #          resourceRequestMemory: \"512Mi\"\n  #          resourceLimitCpu: \"1\"\n  #          resourceLimitMemory: \"1024Mi\"\n\n# Here you can add additional agents\n# They inherit all values from `agent` so you only need to specify values which differ\nadditionalAgents: {}\n#  maven:\n#    podName: maven\n#    customJenkinsLabels: maven\n#    # An example of overriding the jnlp container\n#    # sideContainerName: jnlp\n#    image: jenkins/jnlp-agent-maven\n#    tag: latest\n#  python:\n#    podName: python\n#    customJenkinsLabels: python\n#    sideContainerName: python\n#    image: python\n#    tag: \"3\"\n#    command: \"/bin/sh -c\"\n#    args: \"cat\"\n#    TTYEnabled: true\n\npersistence:\n  enabled: true\n  ## A manually managed Persistent Volume and Claim\n  ## Requires persistence.enabled: true\n  ## If defined, PVC must be created manually before volume will be bound\n  existingClaim:\n  ## jenkins data Persistent Volume Storage Class\n  ## If defined, storageClassName: \u003cstorageClass\u003e\n  ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n  ## If undefined (the default) or set to null, no storageClassName spec is\n  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n  ##   GKE, AWS \u0026 OpenStack)\n  ##\n  storageClass:\n  annotations: {}\n  labels: {}\n  accessMode: \"ReadWriteOnce\"\n  size: \"8Gi\"\n  # Existing data source to clone PVC from\n  # ref: https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource/\n  dataSource:\n  #   name: PVC-NAME\n  #   kind: PersistentVolumeClaim\n  volumes:\n  #  - name: nothing\n  #    emptyDir: {}\n  mounts:\n  #  - mountPath: /var/nothing\n  #    name: nothing\n  #    readOnly: true\n\nnetworkPolicy:\n  # Enable creation of NetworkPolicy resources.\n  enabled: false\n  # For Kubernetes v1.4, v1.5 and v1.6, use 'extensions/v1beta1'\n  # For Kubernetes v1.7, use 'networking.k8s.io/v1'\n  apiVersion: networking.k8s.io/v1\n  # You can allow agents to connect from both within the cluster (from within specific/all namespaces) AND/OR from a given external IP range\n  internalAgents:\n    allowed: true\n    podLabels: {}\n    namespaceLabels: {}\n      # project: myproject\n  externalAgents: {}\n  #   ipCIDR: 172.17.0.0/16\n  #   except:\n  #     - 172.17.1.0/24\n\n## Install Default RBAC roles and bindings\nrbac:\n  create: true\n  readSecrets: false\n\nserviceAccount:\n  create: true\n  # The name of the service account is autogenerated by default\n  name:\n  annotations: {}\n  extraLabels: {}\n  imagePullSecretName:\n\n\nserviceAccountAgent:\n  # Specifies whether a ServiceAccount should be created\n  create: false\n  # The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name:\n  annotations: {}\n  extraLabels: {}\n  imagePullSecretName:\n\n## Backup cronjob configuration\n## Ref: https://github.com/maorfr/kube-tasks\nbackup:\n  # Backup must use RBAC\n  # So by enabling backup you are enabling RBAC specific for backup\n  enabled: false\n  # Used for label app.kubernetes.io/component\n  componentName: \"backup\"\n  # Schedule to run jobs. Must be in cron time format\n  # Ref: https://crontab.guru/\n  schedule: \"0 2 * * *\"\n  labels: {}\n  serviceAccount:\n    create: true\n    name:\n    annotations: {}\n    # Example for authorization to AWS S3 using kube2iam or IRSA\n    # Can also be done using environment variables\n    # iam.amazonaws.com/role: \"jenkins\"\n    # \"eks.amazonaws.com/role-arn\": \"arn:aws:iam::123456789012:role/jenkins-backup\"\n  # Set this to terminate the job that is running/failing continously and set the job status to \"Failed\"\n  activeDeadlineSeconds: \"\"\n  image:\n    repository: \"maorfr/kube-tasks\"\n    tag: \"0.2.0\"\n  imagePullSecretName:\n  # Additional arguments for kube-tasks\n  # Ref: https://github.com/maorfr/kube-tasks#simple-backup\n  extraArgs: []\n  # Add existingSecret for AWS credentials\n  existingSecret: {}\n  ## Example for using an existing secret\n   # jenkinsaws:\n  ## Use this key for AWS access key ID\n     # awsaccesskey: jenkins_aws_access_key\n  ## Use this key for AWS secret access key\n     # awssecretkey: jenkins_aws_secret_key\n  # Add additional environment variables\n   # jenkinsgcp:\n  ## Use this key for GCP credentials\n     # gcpcredentials: credentials.json\n  env: []\n  # Example environment variable required for AWS credentials chain\n  # - name: \"AWS_REGION\"\n  #   value: \"us-east-1\"\n  resources:\n    requests:\n      memory: 1Gi\n      cpu: 1\n    limits:\n      memory: 1Gi\n      cpu: 1\n  # Destination to store the backup artifacts\n  # Supported cloud storage services: AWS S3, Minio S3, Azure Blob Storage, Google Cloud Storage\n  # Additional support can added. Visit this repository for details\n  # Ref: https://github.com/maorfr/skbn\n  destination: \"s3://jenkins-data/backup\"\n  # By enabling only the jenkins_home/jobs folder gets backed up, not the whole jenkins instance\n  onlyJobs: false\n  # Enable backup pod security context (must be `true` if runAsUser or fsGroup are set)\n  usePodSecurityContext: true\n  # When setting runAsUser to a different value than 0 also set fsGroup to the same value:\n  runAsUser: 1000\n  fsGroup: 1000\n  securityContextCapabilities: {}\n  #  drop:\n  #    - NET_RAW\ncronJob:\n  apiVersion: batch/v1\n\ncheckDeprecation: true\n\nawsSecurityGroupPolicies:\n  enabled: false\n  policies:\n    - name: \"\"\n      securityGroupIds: []\n      podSelector: {}\n\n# Here you can configure unit tests values when executing the helm unittest in the CONTRIBUTING.md\nhelmtest:\n  # A testing framework for bash\n  bats:\n    # Bash Automated Testing System (BATS)\n    image: \"bats/bats\"\n    tag: \"1.9.0\"\n"
            ],
            "verify": false,
            "version": "4.6.4",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.68.0",
                "chart": "kube-prometheus-stack",
                "name": "prometheus",
                "namespace": "prometheus",
                "revision": 1,
                "values": "{\"additionalPrometheusRulesMap\":{},\"alertmanager\":{\"alertmanagerSpec\":{\"additionalPeers\":[],\"affinity\":{},\"alertmanagerConfigMatcherStrategy\":{},\"alertmanagerConfigNamespaceSelector\":{},\"alertmanagerConfigSelector\":{},\"alertmanagerConfiguration\":{},\"clusterAdvertiseAddress\":false,\"clusterGossipInterval\":\"\",\"clusterPeerTimeout\":\"\",\"clusterPushpullInterval\":\"\",\"configMaps\":[],\"containers\":[],\"externalUrl\":null,\"forceEnableClusterMode\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/alertmanager\",\"sha\":\"\",\"tag\":\"v0.26.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"minReadySeconds\":0,\"nodeSelector\":{},\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"http-web\",\"priorityClassName\":\"\",\"replicas\":1,\"resources\":{},\"retention\":\"120h\",\"routePrefix\":\"/\",\"scheme\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tlsConfig\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"useExistingSecret\":false,\"volumeMounts\":[],\"volumes\":[],\"web\":{}},\"annotations\":{},\"apiVersion\":\"v2\",\"config\":{\"global\":{\"resolve_timeout\":\"5m\"},\"inhibit_rules\":[{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = critical\"],\"target_matchers\":[\"severity =~ warning|info\"]},{\"equal\":[\"namespace\",\"alertname\"],\"source_matchers\":[\"severity = warning\"],\"target_matchers\":[\"severity = info\"]},{\"equal\":[\"namespace\"],\"source_matchers\":[\"alertname = InfoInhibitor\"],\"target_matchers\":[\"severity = info\"]}],\"receivers\":[{\"name\":\"null\"}],\"route\":{\"group_by\":[\"namespace\"],\"group_interval\":\"5m\",\"group_wait\":\"30s\",\"receiver\":\"null\",\"repeat_interval\":\"12h\",\"routes\":[{\"matchers\":[\"alertname =~ \\\"InfoInhibitor|Watchdog\\\"\"],\"receiver\":\"null\"}]},\"templates\":[\"/etc/alertmanager/config/*.tmpl\"]},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"alertmanager\"}},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"secret\":{\"annotations\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30903,\"port\":9093,\"sessionAffinity\":\"\",\"targetPort\":9093,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"enableHttp2\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerSourceRanges\":[],\"nodePort\":30904,\"port\":9093,\"targetPort\":9093,\"type\":\"ClusterIP\"},\"stringConfig\":\"\",\"templateFiles\":{},\"tplConfig\":false},\"cleanPrometheusOperatorObjectNames\":false,\"commonLabels\":{},\"coreDns\":{\"enabled\":true,\"service\":{\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"targetLimit\":0}},\"crds\":{\"enabled\":true},\"defaultRules\":{\"additionalRuleAnnotations\":{},\"additionalRuleGroupAnnotations\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8s\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleGroupLabels\":{\"alertmanager\":{},\"configReloaders\":{},\"etcd\":{},\"general\":{},\"k8s\":{},\"kubeApiserverAvailability\":{},\"kubeApiserverBurnrate\":{},\"kubeApiserverHistogram\":{},\"kubeApiserverSlos\":{},\"kubeControllerManager\":{},\"kubePrometheusGeneral\":{},\"kubePrometheusNodeRecording\":{},\"kubeProxy\":{},\"kubeSchedulerAlerting\":{},\"kubeSchedulerRecording\":{},\"kubeStateMetrics\":{},\"kubelet\":{},\"kubernetesApps\":{},\"kubernetesResources\":{},\"kubernetesStorage\":{},\"kubernetesSystem\":{},\"network\":{},\"node\":{},\"nodeExporterAlerting\":{},\"nodeExporterRecording\":{},\"prometheus\":{},\"prometheusOperator\":{}},\"additionalRuleLabels\":{},\"annotations\":{},\"appNamespacesTarget\":\".*\",\"create\":true,\"disabled\":{},\"labels\":{},\"rules\":{\"alertmanager\":true,\"configReloaders\":true,\"etcd\":true,\"general\":true,\"k8s\":true,\"kubeApiserverAvailability\":true,\"kubeApiserverBurnrate\":true,\"kubeApiserverHistogram\":true,\"kubeApiserverSlos\":true,\"kubeControllerManager\":true,\"kubePrometheusGeneral\":true,\"kubePrometheusNodeRecording\":true,\"kubeProxy\":true,\"kubeSchedulerAlerting\":true,\"kubeSchedulerRecording\":true,\"kubeStateMetrics\":true,\"kubelet\":true,\"kubernetesApps\":true,\"kubernetesResources\":true,\"kubernetesStorage\":true,\"kubernetesSystem\":true,\"network\":true,\"node\":true,\"nodeExporterAlerting\":true,\"nodeExporterRecording\":true,\"prometheus\":true,\"prometheusOperator\":true,\"windows\":true},\"runbookUrl\":\"https://runbooks.prometheus-operator.dev/runbooks\"},\"extraManifests\":[],\"fullnameOverride\":\"\",\"global\":{\"imagePullSecrets\":[],\"imageRegistry\":\"\",\"rbac\":{\"create\":true,\"createAggregateClusterRoles\":false,\"pspAnnotations\":{},\"pspEnabled\":false}},\"grafana\":{\"additionalDataSources\":[],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":true,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"path\":\"/\",\"tls\":[]},\"namespaceOverride\":\"\",\"rbac\":{\"pspEnabled\":false},\"service\":{\"portName\":\"http-web\"},\"serviceMonitor\":{\"enabled\":true,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"1\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false},\"searchNamespace\":\"ALL\"},\"datasources\":{\"alertmanager\":{\"enabled\":true,\"handleGrafanaManagedAlerts\":false,\"implementation\":\"prometheus\",\"uid\":\"alertmanager\"},\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"exemplarTraceIdDestinations\":{},\"httpMethod\":\"POST\",\"isDefaultDatasource\":true,\"label\":\"grafana_datasource\",\"labelValue\":\"1\",\"uid\":\"prometheus\"}}},\"kube-state-metrics\":{\"namespaceOverride\":\"\",\"prometheus\":{\"monitor\":{\"enabled\":true,\"honorLabels\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"create\":true},\"releaseLabel\":true,\"selfMonitor\":{\"enabled\":false}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"jobLabel\":\"component\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[{\"action\":\"drop\",\"regex\":\"apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\",\"sourceLabels\":[\"__name__\",\"le\"]}],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}},\"targetLimit\":0},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"serverName\":null,\"targetLimit\":0}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"additionalLabels\":{},\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"targetLimit\":0}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":2381,\"targetPort\":2381},\"serviceMonitor\":{\"additionalLabels\":{},\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"keyFile\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"http\",\"serverName\":\"\",\"targetLimit\":0}},\"kubeProxy\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":10249,\"targetPort\":10249},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":false,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"targetLimit\":0}},\"kubeScheduler\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"serverName\":null,\"targetLimit\":0}},\"kubeStateMetrics\":{\"enabled\":true},\"kubeTargetVersionOverride\":\"\",\"kubeVersionOverride\":\"\",\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"additionalLabels\":{},\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[{\"action\":\"drop\",\"regex\":\"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_memory_(mapped_file|swap)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_(file_descriptors|tasks_state|threads_max)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_spec.*\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\".+;\",\"sourceLabels\":[\"id\",\"pod\"]}],\"cAdvisorRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"https\":true,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"action\":\"replace\",\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"sampleLimit\":0,\"targetLimit\":0}},\"kubernetesServiceMonitors\":{\"enabled\":true},\"metrics\":{\"enabled\":true,\"serviceMonitor\":{\"enabled\":true}},\"nameOverride\":\"\",\"namespaceOverride\":\"\",\"nodeExporter\":{\"enabled\":true,\"operatingSystems\":{\"darwin\":{\"enabled\":true},\"linux\":{\"enabled\":true}}},\"prometheus\":{\"additionalPodMonitors\":[],\"additionalRulesForClusterRole\":[],\"additionalServiceMonitors\":[],\"agentMode\":false,\"annotations\":{},\"enabled\":true,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"ingressPerReplica\":{\"annotations\":{},\"enabled\":false,\"hostDomain\":\"\",\"hostPrefix\":\"\",\"labels\":{},\"paths\":[],\"tlsSecretName\":\"\",\"tlsSecretPerReplica\":{\"enabled\":false,\"prefix\":\"prometheus\"}},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"podSecurityPolicy\":{\"allowedCapabilities\":[],\"allowedHostPaths\":[],\"volumes\":[]},\"prometheusSpec\":{\"additionalAlertManagerConfigs\":[],\"additionalAlertManagerConfigsSecret\":{},\"additionalAlertRelabelConfigs\":[],\"additionalAlertRelabelConfigsSecret\":{},\"additionalArgs\":[],\"additionalPrometheusSecretsAnnotations\":{},\"additionalRemoteRead\":[],\"additionalRemoteWrite\":[],\"additionalScrapeConfigs\":[],\"additionalScrapeConfigsSecret\":{},\"affinity\":{},\"alertingEndpoints\":[],\"allowOverlappingBlocks\":false,\"apiserverConfig\":{},\"arbitraryFSAccessThroughSMs\":false,\"configMaps\":[],\"containers\":[],\"disableCompaction\":false,\"enableAdminAPI\":false,\"enableFeatures\":[],\"enableRemoteWriteReceiver\":false,\"enforcedLabelLimit\":false,\"enforcedLabelNameLengthLimit\":false,\"enforcedLabelValueLengthLimit\":false,\"enforcedNamespaceLabel\":\"\",\"enforcedSampleLimit\":false,\"enforcedTargetLimit\":false,\"evaluationInterval\":\"\",\"excludedFromEnforcement\":[],\"exemplars\":\"\",\"externalLabels\":{},\"externalUrl\":\"\",\"hostAliases\":[],\"hostNetwork\":false,\"ignoreNamespaceSelectors\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus/prometheus\",\"sha\":\"\",\"tag\":\"v2.47.0\"},\"initContainers\":[],\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"minReadySeconds\":0,\"nodeSelector\":{},\"overrideHonorLabels\":false,\"overrideHonorTimestamps\":false,\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"podMonitorNamespaceSelector\":{},\"podMonitorSelector\":{},\"podMonitorSelectorNilUsesHelmValues\":true,\"portName\":\"http-web\",\"priorityClassName\":\"\",\"probeNamespaceSelector\":{},\"probeSelector\":{},\"probeSelectorNilUsesHelmValues\":true,\"prometheusExternalLabelName\":\"\",\"prometheusExternalLabelNameClear\":false,\"prometheusRulesExcludedFromEnforce\":[],\"query\":{},\"queryLogFile\":false,\"remoteRead\":[],\"remoteWrite\":[],\"remoteWriteDashboards\":false,\"replicaExternalLabelName\":\"\",\"replicaExternalLabelNameClear\":false,\"replicas\":1,\"resources\":{},\"retention\":\"10d\",\"retentionSize\":\"\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"scrapeConfigNamespaceSelector\":{},\"scrapeConfigSelector\":{},\"scrapeConfigSelectorNilUsesHelmValues\":true,\"scrapeInterval\":\"\",\"scrapeTimeout\":\"\",\"secrets\":[],\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"serviceMonitorNamespaceSelector\":{},\"serviceMonitorSelector\":{},\"serviceMonitorSelectorNilUsesHelmValues\":true,\"shards\":1,\"storageSpec\":{},\"thanos\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"tracingConfig\":{},\"tsdb\":{\"outOfOrderTimeWindow\":\"0s\"},\"version\":\"\",\"volumeMounts\":[],\"volumes\":[],\"walCompression\":true,\"web\":{}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30090,\"port\":9090,\"publishNotReadyAddresses\":false,\"sessionAffinity\":\"\",\"targetPort\":9090,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"servicePerReplica\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"loadBalancerSourceRanges\":[],\"nodePort\":30091,\"port\":9090,\"targetPort\":9090,\"type\":\"ClusterIP\"},\"thanosIngress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"nodePort\":30901,\"paths\":[],\"servicePort\":10901,\"tls\":[]},\"thanosService\":{\"annotations\":{},\"clusterIP\":\"None\",\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"ClusterIP\"},\"thanosServiceExternal\":{\"annotations\":{},\"enabled\":false,\"externalTrafficPolicy\":\"Cluster\",\"httpNodePort\":30902,\"httpPort\":10902,\"httpPortName\":\"http\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30901,\"port\":10901,\"portName\":\"grpc\",\"targetHttpPort\":\"http\",\"targetPort\":\"grpc\",\"type\":\"LoadBalancer\"},\"thanosServiceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"enabled\":false,\"interval\":\"\",\"metricRelabelings\":[],\"relabelings\":[],\"scheme\":\"\",\"tlsConfig\":{}}},\"prometheus-node-exporter\":{\"extraArgs\":[\"--collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\",\"--collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\"],\"namespaceOverride\":\"\",\"podLabels\":{\"jobLabel\":\"node-exporter\"},\"prometheus\":{\"monitor\":{\"enabled\":true,\"interval\":\"\",\"jobLabel\":\"jobLabel\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"targetLimit\":0}},\"rbac\":{\"pspEnabled\":false},\"releaseLabel\":true,\"service\":{\"portName\":\"http-metrics\"}},\"prometheusOperator\":{\"admissionWebhooks\":{\"annotations\":{},\"caBundle\":\"\",\"certManager\":{\"admissionCert\":{\"duration\":\"\"},\"enabled\":false,\"rootCert\":{\"duration\":\"\"}},\"createSecretJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"enabled\":true,\"failurePolicy\":\"\",\"patch\":{\"affinity\":{},\"annotations\":{},\"enabled\":true,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"registry.k8s.io\",\"repository\":\"ingress-nginx/kube-webhook-certgen\",\"sha\":\"\",\"tag\":\"v20221220-controller-v1.5.1-58-g787ea74b6\"},\"nodeSelector\":{},\"podAnnotations\":{},\"priorityClassName\":\"\",\"resources\":{},\"securityContext\":{\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":2000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"tolerations\":[]},\"patchWebhookJob\":{\"securityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true}},\"timeoutSeconds\":10},\"affinity\":{},\"alertmanagerConfigNamespaces\":[],\"alertmanagerInstanceNamespaces\":[],\"alertmanagerInstanceSelector\":\"\",\"annotations\":{},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"denyNamespaces\":[],\"dnsConfig\":{},\"enabled\":true,\"hostNetwork\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-operator\",\"sha\":\"\",\"tag\":\"\"},\"kubeletService\":{\"enabled\":true,\"name\":\"\",\"namespace\":\"kube-system\"},\"labels\":{},\"namespaces\":{},\"networkPolicy\":{\"enabled\":false,\"flavor\":\"kubernetes\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"prometheusConfigReloader\":{\"enableProbe\":false,\"image\":{\"registry\":\"quay.io\",\"repository\":\"prometheus-operator/prometheus-config-reloader\",\"sha\":\"\",\"tag\":\"\"},\"resources\":{\"limits\":{\"cpu\":\"200m\",\"memory\":\"50Mi\"},\"requests\":{\"cpu\":\"200m\",\"memory\":\"50Mi\"}}},\"prometheusInstanceNamespaces\":[],\"prometheusInstanceSelector\":\"\",\"resources\":{},\"revisionHistoryLimit\":10,\"secretFieldSelector\":\"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30080,\"nodePortTls\":30443,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"relabelings\":[],\"sampleLimit\":0,\"scrapeTimeout\":\"\",\"selfMonitor\":true,\"targetLimit\":0},\"thanosImage\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.32.2\"},\"thanosRulerInstanceNamespaces\":[],\"thanosRulerInstanceSelector\":\"\",\"tls\":{\"enabled\":true,\"internalPort\":10250,\"tlsMinVersion\":\"VersionTLS13\"},\"tolerations\":[],\"verticalPodAutoscaler\":{\"controlledResources\":[],\"enabled\":false,\"maxAllowed\":{},\"minAllowed\":{},\"updatePolicy\":{\"updateMode\":\"Auto\"}}},\"thanosRuler\":{\"annotations\":{},\"enabled\":false,\"extraSecret\":{\"annotations\":{},\"data\":{}},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"paths\":[],\"tls\":[]},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":\"\",\"minAvailable\":1},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"externalIPs\":[],\"externalTrafficPolicy\":\"Cluster\",\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"nodePort\":30905,\"port\":10902,\"targetPort\":10902,\"type\":\"ClusterIP\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"name\":\"\"},\"serviceMonitor\":{\"additionalLabels\":{},\"bearerTokenFile\":null,\"interval\":\"\",\"labelLimit\":0,\"labelNameLengthLimit\":0,\"labelValueLengthLimit\":0,\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"sampleLimit\":0,\"scheme\":\"\",\"selfMonitor\":true,\"targetLimit\":0,\"tlsConfig\":{}},\"thanosRulerSpec\":{\"affinity\":{},\"alertmanagersConfig\":{},\"containers\":[],\"evaluationInterval\":\"\",\"externalPrefix\":null,\"image\":{\"registry\":\"quay.io\",\"repository\":\"thanos/thanos\",\"sha\":\"\",\"tag\":\"v0.32.2\"},\"initContainers\":[],\"labels\":{},\"listenLocal\":false,\"logFormat\":\"logfmt\",\"logLevel\":\"info\",\"nodeSelector\":{},\"objectStorageConfig\":{},\"objectStorageConfigFile\":\"\",\"paused\":false,\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podMetadata\":{},\"portName\":\"web\",\"priorityClassName\":\"\",\"queryConfig\":{},\"queryEndpoints\":[],\"replicas\":1,\"resources\":{},\"retention\":\"24h\",\"routePrefix\":\"/\",\"ruleNamespaceSelector\":{},\"ruleSelector\":{},\"ruleSelectorNilUsesHelmValues\":true,\"securityContext\":{\"fsGroup\":2000,\"runAsGroup\":2000,\"runAsNonRoot\":true,\"runAsUser\":1000,\"seccompProfile\":{\"type\":\"RuntimeDefault\"}},\"storage\":{},\"tolerations\":[],\"topologySpreadConstraints\":[],\"volumeMounts\":[],\"volumes\":[]}},\"windowsMonitoring\":{\"enabled\":false,\"job\":\"prometheus-windows-exporter\"}}",
                "version": "51.0.3"
              }
            ],
            "name": "prometheus",
            "namespace": "prometheus",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": true,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "metrics.enabled",
                "type": "",
                "value": "true"
              },
              {
                "name": "metrics.serviceMonitor.enabled",
                "type": "",
                "value": "true"
              }
            ],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# Default values for kube-prometheus-stack.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\n## Provide a name in place of kube-prometheus-stack for `app:` labels\n##\nnameOverride: \"\"\n\n## Override the deployment namespace\n##\nnamespaceOverride: \"\"\n\n## Provide a k8s version to auto dashboard import script example: kubeTargetVersionOverride: 1.26.6\n##\nkubeTargetVersionOverride: \"\"\n\n## Allow kubeVersion to be overridden while creating the ingress\n##\nkubeVersionOverride: \"\"\n\n## Provide a name to substitute for the full names of resources\n##\nfullnameOverride: \"\"\n\n## Labels to apply to all resources\n##\ncommonLabels: {}\n# scmhash: abc123\n# myLabel: aakkmd\n\n## Install Prometheus Operator CRDs\n##\ncrds:\n  enabled: true\n\n## Create default rules for monitoring the cluster\n##\ndefaultRules:\n  create: true\n  rules:\n    alertmanager: true\n    etcd: true\n    configReloaders: true\n    general: true\n    k8s: true\n    kubeApiserverAvailability: true\n    kubeApiserverBurnrate: true\n    kubeApiserverHistogram: true\n    kubeApiserverSlos: true\n    kubeControllerManager: true\n    kubelet: true\n    kubeProxy: true\n    kubePrometheusGeneral: true\n    kubePrometheusNodeRecording: true\n    kubernetesApps: true\n    kubernetesResources: true\n    kubernetesStorage: true\n    kubernetesSystem: true\n    kubeSchedulerAlerting: true\n    kubeSchedulerRecording: true\n    kubeStateMetrics: true\n    network: true\n    node: true\n    nodeExporterAlerting: true\n    nodeExporterRecording: true\n    prometheus: true\n    prometheusOperator: true\n    windows: true\n\n  ## Reduce app namespace alert scope\n  appNamespacesTarget: \".*\"\n\n  ## Labels for default rules\n  labels: {}\n  ## Annotations for default rules\n  annotations: {}\n\n  ## Additional labels for PrometheusRule alerts\n  additionalRuleLabels: {}\n\n  ## Additional annotations for PrometheusRule alerts\n  additionalRuleAnnotations: {}\n\n  ## Additional labels for specific PrometheusRule alert groups\n  additionalRuleGroupLabels:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8s: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  ## Additional annotations for specific PrometheusRule alerts groups\n  additionalRuleGroupAnnotations:\n    alertmanager: {}\n    etcd: {}\n    configReloaders: {}\n    general: {}\n    k8s: {}\n    kubeApiserverAvailability: {}\n    kubeApiserverBurnrate: {}\n    kubeApiserverHistogram: {}\n    kubeApiserverSlos: {}\n    kubeControllerManager: {}\n    kubelet: {}\n    kubeProxy: {}\n    kubePrometheusGeneral: {}\n    kubePrometheusNodeRecording: {}\n    kubernetesApps: {}\n    kubernetesResources: {}\n    kubernetesStorage: {}\n    kubernetesSystem: {}\n    kubeSchedulerAlerting: {}\n    kubeSchedulerRecording: {}\n    kubeStateMetrics: {}\n    network: {}\n    node: {}\n    nodeExporterAlerting: {}\n    nodeExporterRecording: {}\n    prometheus: {}\n    prometheusOperator: {}\n\n  ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.\n  runbookUrl: \"https://runbooks.prometheus-operator.dev/runbooks\"\n\n  ## Disabled PrometheusRule alerts\n  disabled: {}\n  # KubeAPIDown: true\n  # NodeRAIDDegraded: true\n\n## Deprecated way to provide custom recording or alerting rules to be deployed into the cluster.\n##\n# additionalPrometheusRules: []\n#  - name: my-rule-file\n#    groups:\n#      - name: my_group\n#        rules:\n#        - record: my_record\n#          expr: 100 * my_record\n\n## Provide custom recording or alerting rules to be deployed into the cluster.\n##\nadditionalPrometheusRulesMap: {}\n#  rule-name:\n#    groups:\n#    - name: my_group\n#      rules:\n#      - record: my_record\n#        expr: 100 * my_record\n\n##\nglobal:\n  rbac:\n    create: true\n\n    ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs\n    ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles\n    createAggregateClusterRoles: false\n    pspEnabled: false\n    pspAnnotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)\n  ##\n  imageRegistry: \"\"\n\n  ## Reference to one or more secrets to be used when pulling images\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n  ##\n  imagePullSecrets: []\n  # - name: \"image-pull-secret\"\n  # or\n  # - \"image-pull-secret\"\n\nwindowsMonitoring:\n  ## Deploys the windows-exporter and Windows-specific dashboards and rules\n  enabled: false\n  ## Job must match jobLabel in the PodMonitor/ServiceMonitor and is used for the rules\n  job: prometheus-windows-exporter\n\n## Configuration for alertmanager\n## ref: https://prometheus.io/docs/alerting/alertmanager/\n##\nalertmanager:\n\n  ## Deploy alertmanager\n  ##\n  enabled: true\n\n  ## Annotations for Alertmanager\n  ##\n  annotations: {}\n\n  ## Api that prometheus will use to communicate with alertmanager. Possible values are v1, v2\n  ##\n  apiVersion: v2\n\n  ## Service account for Alertmanager to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n    automountServiceAccountToken: true\n\n  ## Configure pod disruption budgets for Alertmanager\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ## Alertmanager configuration directives\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  config:\n    global:\n      resolve_timeout: 5m\n    inhibit_rules:\n      - source_matchers:\n          - 'severity = critical'\n        target_matchers:\n          - 'severity =~ warning|info'\n        equal:\n          - 'namespace'\n          - 'alertname'\n      - source_matchers:\n          - 'severity = warning'\n        target_matchers:\n          - 'severity = info'\n        equal:\n          - 'namespace'\n          - 'alertname'\n      - source_matchers:\n          - 'alertname = InfoInhibitor'\n        target_matchers:\n          - 'severity = info'\n        equal:\n          - 'namespace'\n    route:\n      group_by: ['namespace']\n      group_wait: 30s\n      group_interval: 5m\n      repeat_interval: 12h\n      receiver: 'null'\n      routes:\n      - receiver: 'null'\n        matchers:\n          - alertname =~ \"InfoInhibitor|Watchdog\"\n    receivers:\n    - name: 'null'\n    templates:\n    - '/etc/alertmanager/config/*.tmpl'\n\n  ## Alertmanager configuration directives (as string type, preferred over the config hash map)\n  ## stringConfig will be used only, if tplConfig is true\n  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file\n  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/\n  ##\n  stringConfig: \"\"\n\n  ## Pass the Alertmanager configuration directives through Helm's templating\n  ## engine. If the Alertmanager configuration contains Alertmanager templates,\n  ## they'll need to be properly escaped so that they are not interpreted by\n  ## Helm\n  ## ref: https://helm.sh/docs/developing_charts/#using-the-tpl-function\n  ##      https://prometheus.io/docs/alerting/configuration/#tmpl_string\n  ##      https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  tplConfig: false\n\n  ## Alertmanager template files to format alerts\n  ## By default, templateFiles are placed in /etc/alertmanager/config/ and if\n  ## they have a .tmpl file suffix will be loaded. See config.templates above\n  ## to change, add other suffixes. If adding other suffixes, be sure to update\n  ## config.templates above to include those suffixes.\n  ## ref: https://prometheus.io/docs/alerting/notifications/\n  ##      https://prometheus.io/docs/alerting/notification_examples/\n  ##\n  templateFiles: {}\n  #\n  ## An example template:\n  #   template_1.tmpl: |-\n  #       {{ define \"cluster\" }}{{ .ExternalURL | reReplaceAll \".*alertmanager\\\\.(.*)\" \"$1\" }}{{ end }}\n  #\n  #       {{ define \"slack.myorg.text\" }}\n  #       {{- $root := . -}}\n  #       {{ range .Alerts }}\n  #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`\n  #         *Cluster:* {{ template \"cluster\" $root }}\n  #         *Description:* {{ .Annotations.description }}\n  #         *Graph:* \u003c{{ .GeneratorURL }}|:chart_with_upwards_trend:\u003e\n  #         *Runbook:* \u003c{{ .Annotations.runbook }}|:spiral_note_pad:\u003e\n  #         *Details:*\n  #           {{ range .Labels.SortedPairs }} - *{{ .Name }}:* `{{ .Value }}`\n  #           {{ end }}\n  #       {{ end }}\n  #       {{ end }}\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Override ingress to a different defined port on the service\n    # servicePort: 8081\n    ## Override ingress to a different service then the default, this is useful if you need to\n    ## point to a specific instance of the alertmanager (eg kube-prometheus-stack-alertmanager-0)\n    # serviceName: kube-prometheus-stack-alertmanager-0\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - alertmanager.domain.com\n\n    ## Paths to use for ingress rules - one path should match the alertmanagerSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Alertmanager Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: alertmanager-general-tls\n    #   hosts:\n    #   - alertmanager.example.com\n\n  ## Configuration for Alertmanager secret\n  ##\n  secret:\n    annotations: {}\n\n  ## Configuration for creating an Ingress that will map to each Alertmanager replica service\n  ## alertmanager.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for alertmanager per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"alertmanager\"\n\n  ## Configuration for Alertmanager service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Port for Alertmanager Service to listen on\n    ##\n    port: 9093\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 9093\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30903\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for Alertmanager service\n    additionalPorts: []\n    # additionalPorts:\n    # - name: authenticated\n    #   port: 8081\n    #   targetPort: 8081\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## If you want to make sure that connections from a particular client are passed to the same Pod each time\n    ## Accepts 'ClientIP' or ''\n    ##\n    sessionAffinity: \"\"\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configuration for creating a separate Service for each statefulset Alertmanager replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Alertmanager Service per replica to listen on\n    ##\n    port: 9093\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9093\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30904\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## If true, create a serviceMonitor for alertmanager\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## enableHttp2: Whether to enable HTTP2.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n    enableHttp2: true\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Settings affecting alertmanagerSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec\n  ##\n  alertmanagerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.\n    ##\n    podMetadata: {}\n\n    ## Image of Alertmanager\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/alertmanager\n      tag: v0.26.0\n      sha: \"\"\n\n    ## If true then the user will be responsible to provide a secret with alertmanager configuration\n    ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used\n    ##\n    useExistingSecret: false\n\n    ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the\n    ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.\n    ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.\n    ##\n    configMaps: []\n\n    ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for\n    ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.\n    ##\n    # configSecret:\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerwebspec\n    web: {}\n\n    ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.\n    ##\n    alertmanagerConfigSelector: {}\n    ## Example which selects all alertmanagerConfig resources\n    ## with label \"alertconfig\" with values any of \"example-config\" or \"example-config-2\"\n    # alertmanagerConfigSelector:\n    #   matchExpressions:\n    #     - key: alertconfig\n    #       operator: In\n    #       values:\n    #         - example-config\n    #         - example-config-2\n    #\n    ## Example which selects all alertmanagerConfig resources with label \"role\" set to \"example-config\"\n    # alertmanagerConfigSelector:\n    #   matchLabels:\n    #     role: example-config\n\n    ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.\n    ##\n    alertmanagerConfigNamespaceSelector: {}\n    ## Example which selects all namespaces\n    ## with label \"alertmanagerconfig\" with values any of \"example-namespace\" or \"example-namespace-2\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchExpressions:\n    #     - key: alertmanagerconfig\n    #       operator: In\n    #       values:\n    #         - example-namespace\n    #         - example-namespace-2\n\n    ## Example which selects all namespaces with label \"alertmanagerconfig\" set to \"enabled\"\n    # alertmanagerConfigNamespaceSelector:\n    #   matchLabels:\n    #     alertmanagerconfig: enabled\n\n    ## AlermanagerConfig to be used as top level configuration\n    ##\n    alertmanagerConfiguration: {}\n    ## Example with select a global alertmanagerconfig\n    # alertmanagerConfiguration:\n    #   name: global-alertmanager-Configuration\n\n    ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:\n    ##\n    alertmanagerConfigMatcherStrategy: {}\n    ## Example with use OnNamespace strategy\n    # alertmanagerConfigMatcherStrategy:\n    #   type: OnNamespace\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for Alertmanager to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 120h\n\n    ## Storage is the definition of how storage will be used by the Alertmanager instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #     selector: {}\n\n\n    ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false\n    ##\n    externalUrl:\n\n    ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the alertmanager instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: alertmanager\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the Alertmanager UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.\n    ##\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.3.0\n    #   args:\n    #   - --upstream=http://127.0.0.1:9093\n    #   - --http-address=0.0.0.0:8081\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   resources: {}\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.\n    ##\n    additionalPeers: []\n\n    ## PortName to use for Alert Manager.\n    ##\n    portName: \"http-web\"\n\n    ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918\n    ##\n    clusterAdvertiseAddress: false\n\n    ## clusterGossipInterval determines interval between gossip attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterGossipInterval: \"\"\n\n    ## clusterPeerTimeout determines timeout for cluster peering.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPeerTimeout: \"\"\n\n    ## clusterPushpullInterval determines interval between pushpull attempts.\n    ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)\n    clusterPushpullInterval: \"\"\n\n    ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.\n    ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.\n    forceEnableClusterMode: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml\n##\ngrafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations: {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n      # Allow discovery in all namespaces for dashboards\n      searchNamespace: ALL\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n      isDefaultDatasource: true\n\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      ## Prometheus request timeout in seconds\n      # timeout: 30\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Set method for HTTP to send query to datasource\n      httpMethod: POST\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations: {}\n        # datasourceUid: Jaeger\n        # traceIdLabelName: trace_id\n      alertmanager:\n        enabled: true\n        uid: alertmanager\n        handleGrafanaManagedAlerts: false\n        implementation: prometheus\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources: []\n  # - name: prometheus-sample\n  #   access: proxy\n  #   basicAuth: true\n  #   basicAuthPassword: pass\n  #   basicAuthUser: daco\n  #   editable: false\n  #   jsonData:\n  #       tlsSkipVerify: true\n  #   orgId: 1\n  #   type: prometheus\n  #   url: https://{{ printf \"%s-prometheus.svc\" .Release.Name }}:9090\n  #   version: 1\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Flag to disable all the kubernetes component scrapers\n##\nkubernetesServiceMonitors:\n  enabled: true\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_memory_(mapped_file|swap)'\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_(file_descriptors|tasks_state|threads_max)'\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: 'container_spec.*'\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: '.+;'\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - action: replace\n        sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    port: 9153\n    targetPort: 9153\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2381\n    targetPort: 2381\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kube scheduler\n##\nkubeScheduler:\n  enabled: true\n\n  ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeScheduler.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.23.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-scheduler\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    ## Enable scraping kube-scheduler over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    ## Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    ## Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kube proxy\n##\nkubeProxy:\n  enabled: true\n\n  ## If your kube proxy is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  service:\n    enabled: true\n    port: 10249\n    targetPort: 10249\n    # selector:\n    #   k8s-app: kube-proxy\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-proxy over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks\n    ##\n    https: false\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kube state metrics\n##\nkubeStateMetrics:\n  enabled: true\n\n## Configuration for kube-state-metrics subchart\n##\nkube-state-metrics:\n  namespaceOverride: \"\"\n  rbac:\n    create: true\n  releaseLabel: true\n  prometheus:\n    monitor:\n      enabled: true\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      # Keep labels from scraped data, overriding server-side labels\n      ##\n      honorLabels: true\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - action: keep\n      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n      #   sourceLabels: [__name__]\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n\n  selfMonitor:\n    enabled: false\n\n## Deploy node exporter as a daemonset to all nodes\n##\nnodeExporter:\n  enabled: true\n  operatingSystems:\n    linux:\n      enabled: true\n    darwin:\n      enabled: true\n\n## Configuration for prometheus-node-exporter subchart\n##\nprometheus-node-exporter:\n  namespaceOverride: \"\"\n  podLabels:\n    ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards\n    ##\n    jobLabel: node-exporter\n  releaseLabel: true\n  extraArgs:\n    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$\n  service:\n    portName: http-metrics\n  prometheus:\n    monitor:\n      enabled: true\n\n      jobLabel: jobLabel\n\n      ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n      ##\n      interval: \"\"\n\n      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n      ##\n      sampleLimit: 0\n\n      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n      ##\n      targetLimit: 0\n\n      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelLimit: 0\n\n      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelNameLengthLimit: 0\n\n      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n      ##\n      labelValueLengthLimit: 0\n\n      ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.\n      ##\n      scrapeTimeout: \"\"\n\n      ## proxyUrl: URL of a proxy that should be used for scraping.\n      ##\n      proxyUrl: \"\"\n\n      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      metricRelabelings: []\n      # - sourceLabels: [__name__]\n      #   separator: ;\n      #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+\n      #   replacement: $1\n      #   action: drop\n\n      ## RelabelConfigs to apply to samples before scraping\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n      ##\n      relabelings: []\n      # - sourceLabels: [__meta_kubernetes_pod_node_name]\n      #   separator: ;\n      #   regex: ^(.*)$\n      #   targetLabel: nodename\n      #   replacement: $1\n      #   action: replace\n  rbac:\n    ## If true, create PSPs for node-exporter\n    ##\n    pspEnabled: false\n\n## Manages Prometheus and Alertmanager components\n##\nprometheusOperator:\n  enabled: true\n\n  ## Number of old replicasets to retain ##\n  ## The default value is 10, 0 will garbage-collect old replicasets ##\n  revisionHistoryLimit: 10\n\n  ## Prometheus-Operator v0.39.0 and later support TLS natively.\n  ##\n  tls:\n    enabled: true\n    # Value must match version names from https://golang.org/pkg/crypto/tls/#pkg-constants\n    tlsMinVersion: VersionTLS13\n    # The default webhook port is 10250 in order to work out-of-the-box in GKE private clusters and avoid adding firewall rules.\n    internalPort: 10250\n\n  ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted\n  ## rules from making their way into prometheus and potentially preventing the container from starting\n  admissionWebhooks:\n    ## Valid values: Fail, Ignore, IgnoreOnInstallOnly\n    ## IgnoreOnInstallOnly - If Release.IsInstall returns \"true\", set \"Ignore\" otherwise \"Fail\"\n    failurePolicy: \"\"\n    ## The default timeoutSeconds is 10 and the maximum value is 30.\n    timeoutSeconds: 10\n    enabled: true\n    ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.\n    ## If unspecified, system trust roots on the apiserver are used.\n    caBundle: \"\"\n    ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.\n    ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own\n    ## certs ahead of time if you wish.\n    ##\n    annotations: {}\n    #   argocd.argoproj.io/hook: PreSync\n    #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n    patch:\n      enabled: true\n      image:\n        registry: registry.k8s.io\n        repository: ingress-nginx/kube-webhook-certgen\n        tag: v20221220-controller-v1.5.1-58-g787ea74b6\n        sha: \"\"\n        pullPolicy: IfNotPresent\n      resources: {}\n      ## Provide a priority class name to the webhook patching job\n      ##\n      priorityClassName: \"\"\n      annotations: {}\n      #   argocd.argoproj.io/hook: PreSync\n      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded\n      podAnnotations: {}\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n\n      ## SecurityContext holds pod-level security attributes and common container settings.\n      ## This defaults to non root user with uid 2000 and gid 2000. *v1.PodSecurityContext  false\n      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n      ##\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n\n    # Security context for create job container\n    createSecretJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n\n      # Security context for patch job container\n    patchWebhookJob:\n      securityContext:\n        allowPrivilegeEscalation: false\n        readOnlyRootFilesystem: true\n        capabilities:\n          drop:\n          - ALL\n\n    # Use certmanager to generate webhook certs\n    certManager:\n      enabled: false\n      # self-signed root certificate\n      rootCert:\n        duration: \"\"  # default to be 5y\n      admissionCert:\n        duration: \"\"  # default to be 1y\n      # issuerRef:\n      #   name: \"issuer\"\n      #   kind: \"ClusterIssuer\"\n\n  ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).\n  ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration\n  ##\n  namespaces: {}\n    # releaseNamespace: true\n    # additional:\n    # - kube-system\n\n  ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).\n  ##\n  denyNamespaces: []\n\n  ## Filter namespaces to look for prometheus-operator custom resources\n  ##\n  alertmanagerInstanceNamespaces: []\n  alertmanagerConfigNamespaces: []\n  prometheusInstanceNamespaces: []\n  thanosRulerInstanceNamespaces: []\n\n  ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.\n  ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)\n  ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094\n  ##\n  # clusterDomain: \"cluster.local\"\n\n  networkPolicy:\n    ## Enable creation of NetworkPolicy resources.\n    ##\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   egress:\n\n    ## match labels used in selector\n    # matchLabels: {}\n\n  ## Service account for Prometheus Operator to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n\n  ## Configuration for Prometheus operator service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n  ## Port to expose on each node\n  ## Only used if service.type is 'NodePort'\n  ##\n    nodePort: 30080\n\n    nodePortTls: 30443\n\n  ## Additional ports to open for Prometheus service\n  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services\n  ##\n    additionalPorts: []\n\n  ## Loadbalancer IP\n  ## Only use if service.type is \"LoadBalancer\"\n  ##\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n  ## Service type\n  ## NodePort, ClusterIP, LoadBalancer\n  ##\n    type: ClusterIP\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n  # ## Labels to add to the operator deployment\n  # ##\n  labels: {}\n\n  ## Annotations to add to the operator deployment\n  ##\n  annotations: {}\n\n  ## Labels to add to the operator pod\n  ##\n  podLabels: {}\n\n  ## Annotations to add to the operator pod\n  ##\n  podAnnotations: {}\n\n  ## Assign a PriorityClassName to pods if set\n  # priorityClassName: \"\"\n\n  ## Define Log Format\n  # Use logfmt (default) or json logging\n  # logFormat: logfmt\n\n  ## Decrease log verbosity to errors only\n  # logLevel: error\n\n  ## If true, the operator will create and maintain a service for scraping kubelets\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md\n  ##\n  kubeletService:\n    enabled: true\n    namespace: kube-system\n    ## Use '{{ template \"kube-prometheus-stack.fullname\" . }}-kubelet' by default\n    name: \"\"\n\n  ## Create a servicemonitor for the operator\n  ##\n  serviceMonitor:\n    ## Labels for ServiceMonitor\n    additionalLabels: {}\n\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## Scrape timeout. If not set, the Prometheus default scrape timeout is used.\n    scrapeTimeout: \"\"\n    selfMonitor: true\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Resource limits \u0026 requests\n  ##\n  resources: {}\n  # limits:\n  #   cpu: 200m\n  #   memory: 200Mi\n  # requests:\n  #   cpu: 100m\n  #   memory: 100Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  ## Define which Nodes the Pods are scheduled on.\n  ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Tolerations for use with node taints\n  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n  ##\n  tolerations: []\n  # - key: \"key\"\n  #   operator: \"Equal\"\n  #   value: \"value\"\n  #   effect: \"NoSchedule\"\n\n  ## Assign custom affinity rules to the prometheus operator\n  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n  securityContext:\n    fsGroup: 65534\n    runAsGroup: 65534\n    runAsNonRoot: true\n    runAsUser: 65534\n    seccompProfile:\n      type: RuntimeDefault\n\n  ## Container-specific security context configuration\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n  ##\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n      - ALL\n\n  # Enable vertical pod autoscaler support for prometheus-operator\n  verticalPodAutoscaler:\n    enabled: false\n\n    # Recommender responsible for generating recommendation for the object.\n    # List should be empty (then the default recommender will generate the recommendation)\n    # or contain exactly one recommender.\n    # recommenders:\n    # - name: custom-recommender-performance\n\n    # List of resources that the vertical pod autoscaler can control. Defaults to cpu and memory\n    controlledResources: []\n    # Specifies which resource values should be controlled: RequestsOnly or RequestsAndLimits.\n    # controlledValues: RequestsAndLimits\n\n    # Define the max allowed resources for the pod\n    maxAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n    # Define the min allowed resources for the pod\n    minAllowed: {}\n    # cpu: 200m\n    # memory: 100Mi\n\n    updatePolicy:\n      # Specifies minimal number of replicas which need to be alive for VPA Updater to attempt pod eviction\n      # minReplicas: 1\n      # Specifies whether recommended updates are applied when a Pod is started and whether recommended updates\n      # are applied during the life of a Pod. Possible values are \"Off\", \"Initial\", \"Recreate\", and \"Auto\".\n      updateMode: Auto\n\n  ## Prometheus-operator image\n  ##\n  image:\n    registry: quay.io\n    repository: prometheus-operator/prometheus-operator\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    sha: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus image to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImage: prometheus/prometheus\n\n  ## Prometheus image registry to use for prometheuses managed by the operator\n  ##\n  # prometheusDefaultBaseImageRegistry: quay.io\n\n  ## Alertmanager image to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImage: prometheus/alertmanager\n\n  ## Alertmanager image registry to use for alertmanagers managed by the operator\n  ##\n  # alertmanagerDefaultBaseImageRegistry: quay.io\n\n  ## Prometheus-config-reloader\n  ##\n  prometheusConfigReloader:\n    image:\n      registry: quay.io\n      repository: prometheus-operator/prometheus-config-reloader\n      # if not set appVersion field from Chart.yaml is used\n      tag: \"\"\n      sha: \"\"\n\n    # add prometheus config reloader liveness and readiness probe. Default: false\n    enableProbe: false\n\n    # resource config for prometheusConfigReloader\n    resources:\n      requests:\n        cpu: 200m\n        memory: 50Mi\n      limits:\n        cpu: 200m\n        memory: 50Mi\n\n  ## Thanos side-car image when configured\n  ##\n  thanosImage:\n    registry: quay.io\n    repository: thanos/thanos\n    tag: v0.32.2\n    sha: \"\"\n\n  ## Set a Label Selector to filter watched prometheus and prometheusAgent\n  ##\n  prometheusInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched alertmanager\n  ##\n  alertmanagerInstanceSelector: \"\"\n\n  ## Set a Label Selector to filter watched thanosRuler\n  thanosRulerInstanceSelector: \"\"\n\n  ## Set a Field Selector to filter watched secrets\n  ##\n  secretFieldSelector: \"type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\"\n\n## Deploy a Prometheus instance\n##\nprometheus:\n  enabled: true\n\n  ## Toggle prometheus into agent mode\n  ## Note many of features described below (e.g. rules, query, alerting, remote read, thanos) will not work in agent mode.\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/designs/prometheus-agent.md\n  ##\n  agentMode: false\n\n  ## Annotations for Prometheus\n  ##\n  annotations: {}\n\n  ## Configure network policy for the prometheus\n  networkPolicy:\n    enabled: false\n\n    ## Flavor of the network policy to use.\n    #  Can be:\n    #  * kubernetes for networking.k8s.io/v1/NetworkPolicy\n    #  * cilium     for cilium.io/v2/CiliumNetworkPolicy\n    flavor: kubernetes\n\n    # cilium:\n    #   endpointSelector:\n    #   egress:\n    #   ingress:\n\n    # egress:\n    # - {}\n    # ingress:\n    # - {}\n    # podSelector:\n    #   matchLabels:\n    #     app: prometheus\n\n  ## Service account for Prometheuses to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  # Service for thanos service discovery on sidecar\n  # Enable this can make Thanos Query can use\n  # `--store=dnssrv+_grpc._tcp.${kube-prometheus-stack.fullname}-thanos-discovery.${namespace}.svc.cluster.local` to discovery\n  # Thanos sidecar on prometheus nodes\n  # (Please remember to change ${kube-prometheus-stack.fullname} and ${namespace}. Not just copy and paste!)\n  thanosService:\n    enabled: false\n    annotations: {}\n    labels: {}\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## ClusterIP to assign\n    # Default is to make this a headless service (\"None\")\n    clusterIP: \"None\"\n\n    ## Port to expose on each node, if service type is NodePort\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  # ServiceMonitor to scrape Sidecar metrics\n  # Needs thanosService to be enabled as well\n  thanosServiceMonitor:\n    enabled: false\n    interval: \"\"\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    metricRelabelings: []\n\n    ## relabel configs to apply to samples before ingestion.\n    relabelings: []\n\n  # Service for external access to sidecar\n  # Enabling this creates a service to expose thanos-sidecar outside the cluster.\n  thanosServiceExternal:\n    enabled: false\n    annotations: {}\n    labels: {}\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## gRPC port config\n    portName: grpc\n    port: 10901\n    targetPort: \"grpc\"\n\n    ## HTTP port config (for metrics)\n    httpPortName: http\n    httpPort: 10902\n    targetHttpPort: \"http\"\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: LoadBalancer\n\n    ## Port to expose on each node\n    ##\n    nodePort: 30901\n    httpNodePort: 30902\n\n  ## Configuration for Prometheus service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Port for Prometheus Service to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30090\n\n    ## Loadbalancer IP\n    ## Only use if service.type is \"LoadBalancer\"\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n    ## Additional port to define in the Service\n    additionalPorts: []\n    # additionalPorts:\n    # - name: authenticated\n    #   port: 8081\n    #   targetPort: 8081\n\n    ## Consider that all endpoints are considered \"ready\" even if the Pods themselves are not\n    ## Ref: https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/#ServiceSpec\n    publishNotReadyAddresses: false\n\n    sessionAffinity: \"\"\n\n  ## Configuration for creating a separate Service for each statefulset Prometheus replica\n  ##\n  servicePerReplica:\n    enabled: false\n    annotations: {}\n\n    ## Port for Prometheus Service per replica to listen on\n    ##\n    port: 9090\n\n    ## To be used with a proxy extraContainer port\n    targetPort: 9090\n\n    ## Port to expose on each node\n    ## Only used if servicePerReplica.type is 'NodePort'\n    ##\n    nodePort: 30091\n\n    ## Loadbalancer source IP ranges\n    ## Only used if servicePerReplica.type is \"LoadBalancer\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## Configure pod disruption budgets for Prometheus\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  # Ingress exposes thanos sidecar outside the cluster\n  thanosIngress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n    servicePort: 10901\n\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30901\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanos-gateway.domain.com\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Thanos Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanos-gateway-tls\n    #   hosts:\n    #   - thanos-gateway.domain.com\n    #\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Redirect ingress to an additional defined port on the service\n    # servicePort: 8081\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enabled.\n    ##\n    # hosts:\n    #   - prometheus.domain.com\n    hosts: []\n\n    ## Paths to use for ingress rules - one path should match the prometheusSpec.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for Prometheus Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n      # - secretName: prometheus-general-tls\n      #   hosts:\n      #     - prometheus.example.com\n\n  ## Configuration for creating an Ingress that will map to each Prometheus replica service\n  ## prometheus.servicePerReplica must be enabled\n  ##\n  ingressPerReplica:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n    labels: {}\n\n    ## Final form of the hostname for each per replica ingress is\n    ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}\n    ##\n    ## Prefix for the per replica ingress that will have `-$replicaNumber`\n    ## appended to the end\n    hostPrefix: \"\"\n    ## Domain that will be used for the per replica ingress\n    hostDomain: \"\"\n\n    ## Paths to use for ingress rules\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## Secret name containing the TLS certificate for Prometheus per replica ingress\n    ## Secret must be manually created in the namespace\n    tlsSecretName: \"\"\n\n    ## Separated secret for each per replica Ingress. Can be used together with cert-manager\n    ##\n    tlsSecretPerReplica:\n      enabled: false\n      ## Final form of the secret for each per replica ingress is\n      ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}\n      ##\n      prefix: \"prometheus\"\n\n  ## Configure additional options for default pod security policy for Prometheus\n  ## ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  podSecurityPolicy:\n    allowedCapabilities: []\n    allowedHostPaths: []\n    volumes: []\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## Metric relabel configs to apply to samples before ingestion.\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    #   relabel configs to apply to samples before ingestion.\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Settings affecting prometheusSpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec\n  ##\n  prometheusSpec:\n    ## If true, pass --storage.tsdb.max-block-duration=2h to prometheus. This is already done if using Thanos\n    ##\n    disableCompaction: false\n    ## APIServerConfig\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#apiserverconfig\n    ##\n    apiserverConfig: {}\n\n    ## Allows setting additional arguments for the Prometheus container\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#monitoring.coreos.com/v1.Prometheus\n    additionalArgs: []\n\n    ## Interval between consecutive scrapes.\n    ## Defaults to 30s.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183\n    ##\n    scrapeInterval: \"\"\n\n    ## Number of seconds to wait for target to respond before erroring\n    ##\n    scrapeTimeout: \"\"\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## ListenLocal makes the Prometheus server listen on loopback, so that it does not bind against the Pod IP.\n    ##\n    listenLocal: false\n\n    ## EnableAdminAPI enables Prometheus the administrative HTTP API which includes functionality such as deleting time series.\n    ## This is disabled by default.\n    ## ref: https://prometheus.io/docs/prometheus/latest/querying/api/#tsdb-admin-apis\n    ##\n    enableAdminAPI: false\n\n    ## Sets version of Prometheus overriding the Prometheus version as derived\n    ## from the image tag. Useful in cases where the tag does not follow semver v2.\n    version: \"\"\n\n    ## WebTLSConfig defines the TLS parameters for HTTPS\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#webtlsconfig\n    web: {}\n\n    ## Exemplars related settings that are runtime reloadable.\n    ## It requires to enable the exemplar storage feature to be effective.\n    exemplars: \"\"\n      ## Maximum number of exemplars stored in memory for all series.\n      ## If not set, Prometheus uses its default value.\n      ## A value of zero or less than zero disables the storage.\n      # maxSize: 100000\n\n    # EnableFeatures API enables access to Prometheus disabled features.\n    # ref: https://prometheus.io/docs/prometheus/latest/disabled_features/\n    enableFeatures: []\n    # - exemplar-storage\n\n    ## Image of Prometheus.\n    ##\n    image:\n      registry: quay.io\n      repository: prometheus/prometheus\n      tag: v2.47.0\n      sha: \"\"\n\n    ## Tolerations for use with node taints\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    #  - key: \"key\"\n    #    operator: \"Equal\"\n    #    value: \"value\"\n    #    effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: prometheus\n\n    ## Alertmanagers to which alerts will be sent\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerendpoints\n    ##\n    ## Default configuration will connect to the alertmanager deployed as part of this release\n    ##\n    alertingEndpoints: []\n    # - name: \"\"\n    #   namespace: \"\"\n    #   port: http\n    #   scheme: http\n    #   pathPrefix: \"\"\n    #   tlsConfig: {}\n    #   bearerTokenFile: \"\"\n    #   apiVersion: v2\n\n    ## External labels to add to any time series or alerts when communicating with external systems\n    ##\n    externalLabels: {}\n\n    ## enable --web.enable-remote-write-receiver flag on prometheus-server\n    ##\n    enableRemoteWriteReceiver: false\n\n    ## Name of the external label used to denote replica name\n    ##\n    replicaExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote replica name\n    ##\n    replicaExternalLabelNameClear: false\n\n    ## Name of the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelName: \"\"\n\n    ## If true, the Operator won't add the external label used to denote Prometheus instance name\n    ##\n    prometheusExternalLabelNameClear: false\n\n    ## External URL at which Prometheus will be reachable.\n    ##\n    externalUrl: \"\"\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not\n    ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated\n    ## with the new list of secrets.\n    ##\n    secrets: []\n\n    ## ConfigMaps is a list of ConfigMaps in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.\n    ## The ConfigMaps are mounted into /etc/prometheus/configmaps/.\n    ##\n    configMaps: []\n\n    ## QuerySpec defines the query command line flags when starting Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#queryspec\n    ##\n    query: {}\n\n    ## If nil, select own namespace. Namespaces to be selected for PrometheusRules discovery.\n    ruleNamespaceSelector: {}\n    ## Example which selects PrometheusRules in namespaces with label \"prometheus\" set to \"somelabel\"\n    # ruleNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the servicemonitors created\n    ##\n    serviceMonitorSelectorNilUsesHelmValues: true\n\n    ## ServiceMonitors to be selected for target discovery.\n    ## If {}, select all ServiceMonitors\n    ##\n    serviceMonitorSelector: {}\n    ## Example which selects ServiceMonitors with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## Namespaces to be selected for ServiceMonitor discovery.\n    ##\n    serviceMonitorNamespaceSelector: {}\n    ## Example which selects ServiceMonitors in namespaces with label \"prometheus\" set to \"somelabel\"\n    # serviceMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.podMonitorSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the podmonitors created\n    ##\n    podMonitorSelectorNilUsesHelmValues: true\n\n    ## PodMonitors to be selected for target discovery.\n    ## If {}, select all PodMonitors\n    ##\n    podMonitorSelector: {}\n    ## Example which selects PodMonitors with label \"prometheus\" set to \"somelabel\"\n    # podMonitorSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for PodMonitor discovery.\n    podMonitorNamespaceSelector: {}\n    ## Example which selects PodMonitor in namespaces with label \"prometheus\" set to \"somelabel\"\n    # podMonitorNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.probeSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the probes created\n    ##\n    probeSelectorNilUsesHelmValues: true\n\n    ## Probes to be selected for target discovery.\n    ## If {}, select all Probes\n    ##\n    probeSelector: {}\n    ## Example which selects Probes with label \"prometheus\" set to \"somelabel\"\n    # probeSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for Probe discovery.\n    probeNamespaceSelector: {}\n    ## Example which selects Probe in namespaces with label \"prometheus\" set to \"somelabel\"\n    # probeNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If true, a nil or {} value for prometheus.prometheusSpec.scrapeConfigSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the scrapeConfigs created\n    ##\n    scrapeConfigSelectorNilUsesHelmValues: true\n\n    ## scrapeConfigs to be selected for target discovery.\n    ## If {}, select all scrapeConfigs\n    ##\n    scrapeConfigSelector: {}\n    ## Example which selects scrapeConfigs with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfig:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## If nil, select own namespace. Namespaces to be selected for scrapeConfig discovery.\n    scrapeConfigNamespaceSelector: {}\n    ## Example which selects scrapeConfig in namespaces with label \"prometheus\" set to \"somelabel\"\n    # scrapeConfigNamespaceSelector:\n    #   matchLabels:\n    #     prometheus: somelabel\n\n    ## How long to retain metrics\n    ##\n    retention: 10d\n\n    ## Maximum size of metrics\n    ##\n    retentionSize: \"\"\n\n    ## Allow out-of-order/out-of-bounds samples ingested into Prometheus for a specified duration\n    ## See https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\n    tsdb:\n      outOfOrderTimeWindow: 0s\n\n    ## Enable compression of the write-ahead log using Snappy.\n    ##\n    walCompression: true\n\n    ## If true, the Operator won't process any Prometheus configuration changes\n    ##\n    paused: false\n\n    ## Number of replicas of each shard to deploy for a Prometheus deployment.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ##\n    replicas: 1\n\n    ## EXPERIMENTAL: Number of shards to distribute targets onto.\n    ## Number of replicas multiplied by shards is the total number of Pods created.\n    ## Note that scaling down shards will not reshard data onto remaining instances, it must be manually moved.\n    ## Increasing shards will not reshard data either but it will continue to be available from the same instances.\n    ## To query globally use Thanos sidecar and Thanos querier or remote write data to a central location.\n    ## Sharding is done on the content of the `__address__` target meta-label.\n    ##\n    shards: 1\n\n    ## Log level for Prometheus be configured in\n    ##\n    logLevel: info\n\n    ## Log format for Prometheus be configured in\n    ##\n    logFormat: logfmt\n\n    ## Prefix used to register routes, overriding externalUrl route.\n    ## Useful for proxies that rewrite URLs.\n    ##\n    routePrefix: /\n\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the prometheus pods.\n    ##\n    podMetadata: {}\n    # labels:\n    #   app: prometheus\n    #   k8s-app: prometheus\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the prometheus instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## The remote_read spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotereadspec\n    remoteRead: []\n    # - url: http://remote1/read\n    ## additionalRemoteRead is appended to remoteRead\n    additionalRemoteRead: []\n\n    ## The remote_write spec configuration for Prometheus.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#remotewritespec\n    remoteWrite: []\n    # - url: http://remote1/push\n    ## additionalRemoteWrite is appended to remoteWrite\n    additionalRemoteWrite: []\n\n    ## Enable/Disable Grafana dashboards provisioning for prometheus remote write feature\n    remoteWriteDashboards: false\n\n    ## Resource limits \u0026 requests\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Prometheus StorageSpec for persistent data\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storageSpec: {}\n    ## Using PersistentVolumeClaim\n    ##\n    #  volumeClaimTemplate:\n    #    spec:\n    #      storageClassName: gluster\n    #      accessModes: [\"ReadWriteOnce\"]\n    #      resources:\n    #        requests:\n    #          storage: 50Gi\n    #    selector: {}\n\n    ## Using tmpfs volume\n    ##\n    #  emptyDir:\n    #    medium: Memory\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations\n    ## are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form\n    ## as specified in the official Prometheus documentation:\n    ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are\n    ## appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility\n    ## to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible\n    ## scrape configs are going to break Prometheus after the upgrade.\n    ## AdditionalScrapeConfigs can be defined as a list or as a templated string.\n    ##\n    ## The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the\n    ## port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes\n    ##\n    additionalScrapeConfigs: []\n    # - job_name: kube-etcd\n    #   kubernetes_sd_configs:\n    #     - role: node\n    #   scheme: https\n    #   tls_config:\n    #     ca_file:   /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n    #     cert_file: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n    #     key_file:  /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n    #   relabel_configs:\n    #   - action: labelmap\n    #     regex: __meta_kubernetes_node_label_(.+)\n    #   - source_labels: [__address__]\n    #     action: replace\n    #     targetLabel: __address__\n    #     regex: ([^:;]+):(\\d+)\n    #     replacement: ${1}:2379\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: keep\n    #     regex: .*mst.*\n    #   - source_labels: [__meta_kubernetes_node_name]\n    #     action: replace\n    #     targetLabel: node\n    #     regex: (.*)\n    #     replacement: ${1}\n    #   metric_relabel_configs:\n    #   - regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n    #     action: labeldrop\n    #\n    ## If scrape config contains a repetitive section, you may want to use a template.\n    ## In the following example, you can see how to define `gce_sd_configs` for multiple zones\n    # additionalScrapeConfigs: |\n    #  - job_name: \"node-exporter\"\n    #    gce_sd_configs:\n    #    {{range $zone := .Values.gcp_zones}}\n    #    - project: \"project1\"\n    #      zone: \"{{$zone}}\"\n    #      port: 9100\n    #    {{end}}\n    #    relabel_configs:\n    #    ...\n\n\n    ## If additional scrape configurations are already deployed in a single secret file you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalScrapeConfigs\n    additionalScrapeConfigsSecret: {}\n      # enabled: false\n      # name:\n      # key:\n\n    ## additionalPrometheusSecretsAnnotations allows to add annotations to the kubernetes secret. This can be useful\n    ## when deploying via spinnaker to disable versioning on the secret, strategy.spinnaker.io/versioned: 'false'\n    additionalPrometheusSecretsAnnotations: {}\n\n    ## AdditionalAlertManagerConfigs allows for manual configuration of alertmanager jobs in the form as specified\n    ## in the official Prometheus documentation https://prometheus.io/docs/prometheus/latest/configuration/configuration/#\u003calertmanager_config\u003e.\n    ## AlertManager configurations specified are appended to the configurations generated by the Prometheus Operator.\n    ## As AlertManager configs are appended, the user is responsible to make sure it is valid. Note that using this\n    ## feature may expose the possibility to break upgrades of Prometheus. It is advised to review Prometheus release\n    ## notes to ensure that no incompatible AlertManager configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertManagerConfigs: []\n    # - consul_sd_configs:\n    #   - server: consul.dev.test:8500\n    #     scheme: http\n    #     datacenter: dev\n    #     tag_separator: ','\n    #     services:\n    #       - metrics-prometheus-alertmanager\n\n    ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertManagerConfigs\n    additionalAlertManagerConfigsSecret: {}\n      # name:\n      # key:\n      # optional: false\n\n    ## AdditionalAlertRelabelConfigs allows specifying Prometheus alert relabel configurations. Alert relabel configurations specified are appended\n    ## to the configurations generated by the Prometheus Operator. Alert relabel configurations specified must have the form as specified in the\n    ## official Prometheus documentation: https://prometheus.io/docs/prometheus/latest/configuration/configuration/#alert_relabel_configs.\n    ## As alert relabel configs are appended, the user is responsible to make sure it is valid. Note that using this feature may expose the\n    ## possibility to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible alert relabel\n    ## configs are going to break Prometheus after the upgrade.\n    ##\n    additionalAlertRelabelConfigs: []\n    # - separator: ;\n    #   regex: prometheus_replica\n    #   replacement: $1\n    #   action: labeldrop\n\n    ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage\n    ## them separately from the helm deployment, you can use this section.\n    ## Expected values are the secret name and key\n    ## Cannot be used with additionalAlertRelabelConfigs\n    additionalAlertRelabelConfigsSecret: {}\n      # name:\n      # key:\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000.\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## Thanos configuration allows configuring various aspects of a Prometheus server in a Thanos environment.\n    ## This section is experimental, it may change significantly without deprecation notice in any release.\n    ## This is experimental and may change significantly without backward compatibility in any release.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosspec\n    ##\n    thanos: {}\n      # secretProviderClass:\n      #   provider: gcp\n      #   parameters:\n      #     secrets: |\n      #       - resourceName: \"projects/$PROJECT_ID/secrets/testsecret/versions/latest\"\n      #         fileName: \"objstore.yaml\"\n      # objectStorageConfigFile: /var/secrets/object-store.yaml\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to a Prometheus pod.\n    ## if using proxy extraContainer update targetPort with proxy container port\n    containers: []\n    # containers:\n    # - name: oauth-proxy\n    #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.3.0\n    #   args:\n    #   - --upstream=http://127.0.0.1:9093\n    #   - --http-address=0.0.0.0:8081\n    #   - ...\n    #   ports:\n    #   - containerPort: 8081\n    #     name: oauth-proxy\n    #     protocol: TCP\n    #   resources: {}\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## PortName to use for Prometheus.\n    ##\n    portName: \"http-web\"\n\n    ## ArbitraryFSAccessThroughSMs configures whether configuration based on a service monitor can access arbitrary files\n    ## on the file system of the Prometheus container e.g. bearer token files.\n    arbitraryFSAccessThroughSMs: false\n\n    ## OverrideHonorLabels if set to true overrides all user configured honor_labels. If HonorLabels is set in ServiceMonitor\n    ## or PodMonitor to true, this overrides honor_labels to false.\n    overrideHonorLabels: false\n\n    ## OverrideHonorTimestamps allows to globally enforce honoring timestamps in all scrape configs.\n    overrideHonorTimestamps: false\n\n    ## IgnoreNamespaceSelectors if set to true will ignore NamespaceSelector settings from the podmonitor and servicemonitor\n    ## configs, and they will only discover endpoints within their current namespace. Defaults to false.\n    ignoreNamespaceSelectors: false\n\n    ## EnforcedNamespaceLabel enforces adding a namespace label of origin for each alert and metric that is user created.\n    ## The label value will always be the namespace of the object that is being created.\n    ## Disabled by default\n    enforcedNamespaceLabel: \"\"\n\n    ## PrometheusRulesExcludedFromEnforce - list of prometheus rules to be excluded from enforcing of adding namespace labels.\n    ## Works only if enforcedNamespaceLabel set to true. Make sure both ruleNamespace and ruleName are set for each pair\n    ## Deprecated, use `excludedFromEnforcement` instead\n    prometheusRulesExcludedFromEnforce: []\n\n    ## ExcludedFromEnforcement - list of object references to PodMonitor, ServiceMonitor, Probe and PrometheusRule objects\n    ## to be excluded from enforcing a namespace label of origin.\n    ## Works only if enforcedNamespaceLabel set to true.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#objectreference\n    excludedFromEnforcement: []\n\n    ## QueryLogFile specifies the file to which PromQL queries are logged. Note that this location must be writable,\n    ## and can be persisted using an attached volume. Alternatively, the location can be set to a stdout location such\n    ## as /dev/stdout to log querie information to the default Prometheus log stream. This is only available in versions\n    ## of Prometheus \u003e= 2.16.0. For more details, see the Prometheus docs (https://prometheus.io/docs/guides/query-log/)\n    queryLogFile: false\n\n    ## EnforcedSampleLimit defines global limit on number of scraped samples that will be accepted. This overrides any SampleLimit\n    ## set per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the SampleLimit to keep overall\n    ## number of samples/series under the desired limit. Note that if SampleLimit is lower that value will be taken instead.\n    enforcedSampleLimit: false\n\n    ## EnforcedTargetLimit defines a global limit on the number of scraped targets. This overrides any TargetLimit set\n    ## per ServiceMonitor or/and PodMonitor. It is meant to be used by admins to enforce the TargetLimit to keep the overall\n    ## number of targets under the desired limit. Note that if TargetLimit is lower, that value will be taken instead, except\n    ## if either value is zero, in which case the non-zero value will be used. If both values are zero, no limit is enforced.\n    enforcedTargetLimit: false\n\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. If more than this number of labels are present\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelLimit: false\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. If a label name is longer than this number\n    ## post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus versions\n    ## 2.27.0 and newer.\n    enforcedLabelNameLengthLimit: false\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. If a label value is longer than this\n    ## number post metric-relabeling, the entire scrape will be treated as failed. 0 means no limit. Only valid in Prometheus\n    ## versions 2.27.0 and newer.\n    enforcedLabelValueLengthLimit: false\n\n    ## AllowOverlappingBlocks enables vertical compaction and vertical query merge in Prometheus. This is still experimental\n    ## in Prometheus so it may change in any upcoming release.\n    allowOverlappingBlocks: false\n\n    ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to\n    ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).\n    minReadySeconds: 0\n\n    # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n    # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n    # Use the host's network namespace if true. Make sure to understand the security implications if you want to enable it.\n    # When hostNetwork is enabled, this will set dnsPolicy to ClusterFirstWithHostNet automatically.\n    hostNetwork: false\n\n    # HostAlias holds the mapping between IP and hostnames that will be injected\n    # as an entry in the pod’s hosts file.\n    hostAliases: []\n    #  - ip: 10.10.0.100\n    #    hostnames:\n    #      - a1.app.local\n    #      - b1.app.local\n\n    ## TracingConfig configures tracing in Prometheus.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheustracingconfig\n    tracingConfig: {}\n\n  additionalRulesForClusterRole: []\n  #  - apiGroups: [ \"\" ]\n  #    resources:\n  #      - nodes/proxy\n  #    verbs: [ \"get\", \"list\", \"watch\" ]\n\n  additionalServiceMonitors: []\n  ## Name of the ServiceMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the ServiceMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Service label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the service name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## labels to transfer from the kubernetes service to the target\n    ##\n    # targetLabels: []\n\n    ## labels to transfer from the kubernetes pods to the target\n    ##\n    # podTargetLabels: []\n\n    ## Label selector for services to which this ServiceMonitor applies\n    ##\n    # selector: {}\n\n    ## Namespaces from which services are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected service to be monitored\n    ##\n    # endpoints: []\n      ## Name of the endpoint's service port\n      ## Mutually exclusive with targetPort\n      # - port: \"\"\n\n      ## Name or number of the endpoint's target port\n      ## Mutually exclusive with port\n      # - targetPort: \"\"\n\n      ## File containing bearer token to be used when scraping targets\n      ##\n      #   bearerTokenFile: \"\"\n\n      ## Interval at which metrics should be scraped\n      ##\n      #   interval: 30s\n\n      ## HTTP path to scrape for metrics\n      ##\n      #   path: /metrics\n\n      ## HTTP scheme to use for scraping\n      ##\n      #   scheme: http\n\n      ## TLS configuration to use when scraping the endpoint\n      ##\n      #   tlsConfig:\n\n          ## Path to the CA file\n          ##\n          # caFile: \"\"\n\n          ## Path to client certificate file\n          ##\n          # certFile: \"\"\n\n          ## Skip certificate verification\n          ##\n          # insecureSkipVerify: false\n\n          ## Path to client key file\n          ##\n          # keyFile: \"\"\n\n          ## Server name used to verify host name\n          ##\n          # serverName: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    # relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  additionalPodMonitors: []\n  ## Name of the PodMonitor to create\n  ##\n  # - name: \"\"\n\n    ## Additional labels to set used for the PodMonitorSelector. Together with standard labels from\n    ## the chart\n    ##\n    # additionalLabels: {}\n\n    ## Pod label for use in assembling a job name of the form \u003clabel value\u003e-\u003cport\u003e\n    ## If no label is specified, the pod endpoint name is used.\n    ##\n    # jobLabel: \"\"\n\n    ## Label selector for pods to which this PodMonitor applies\n    ##\n    # selector: {}\n\n    ## PodTargetLabels transfers labels on the Kubernetes Pod onto the target.\n    ##\n    # podTargetLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    # sampleLimit: 0\n\n    ## Namespaces from which pods are selected\n    ##\n    # namespaceSelector:\n      ## Match any namespace\n      ##\n      # any: false\n\n      ## Explicit list of namespace names to select\n      ##\n      # matchNames: []\n\n    ## Endpoints of the selected pods to be monitored\n    ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#podmetricsendpoint\n    ##\n    # podMetricsEndpoints: []\n\n## Configuration for thanosRuler\n## ref: https://thanos.io/tip/components/rule.md/\n##\nthanosRuler:\n\n  ## Deploy thanosRuler\n  ##\n  enabled: false\n\n  ## Annotations for ThanosRuler\n  ##\n  annotations: {}\n\n  ## Service account for ThanosRuler to use.\n  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n  ##\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n\n  ## Configure pod disruption budgets for ThanosRuler\n  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/#specifying-a-poddisruptionbudget\n  ## This configuration is immutable once created and will require the PDB to be deleted to be changed\n  ## https://github.com/kubernetes/kubernetes/issues/45398\n  ##\n  podDisruptionBudget:\n    enabled: false\n    minAvailable: 1\n    maxUnavailable: \"\"\n\n  ingress:\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    annotations: {}\n\n    labels: {}\n\n    ## Hosts must be provided if Ingress is enabled.\n    ##\n    hosts: []\n      # - thanosruler.domain.com\n\n    ## Paths to use for ingress rules - one path should match the thanosruler.routePrefix\n    ##\n    paths: []\n    # - /\n\n    ## For Kubernetes \u003e= 1.18 you should specify the pathType (determines how Ingress paths should be matched)\n    ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types\n    # pathType: ImplementationSpecific\n\n    ## TLS configuration for ThanosRuler Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: thanosruler-general-tls\n    #   hosts:\n    #   - thanosruler.example.com\n\n  ## Configuration for ThanosRuler service\n  ##\n  service:\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## Port for ThanosRuler Service to listen on\n    ##\n    port: 10902\n    ## To be used with a proxy extraContainer port\n    ##\n    targetPort: 10902\n    ## Port to expose on each node\n    ## Only used if service.type is 'NodePort'\n    ##\n    nodePort: 30905\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n\n    ## Additional ports to open for ThanosRuler service\n    additionalPorts: []\n\n    externalIPs: []\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n\n    ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints\n    ##\n    externalTrafficPolicy: Cluster\n\n    ## Service type\n    ##\n    type: ClusterIP\n\n  ## If true, create a serviceMonitor for thanosRuler\n  ##\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    selfMonitor: true\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n\n    ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.\n    ##\n    sampleLimit: 0\n\n    ## TargetLimit defines a limit on the number of scraped targets that will be accepted.\n    ##\n    targetLimit: 0\n\n    ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelLimit: 0\n\n    ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelNameLengthLimit: 0\n\n    ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.\n    ##\n    labelValueLengthLimit: 0\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.\n    scheme: \"\"\n\n    ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.\n    ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api.md#tlsconfig\n    tlsConfig: {}\n\n    bearerTokenFile:\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n  ## Settings affecting thanosRulerpec\n  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#thanosrulerspec\n  ##\n  thanosRulerSpec:\n    ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ## Metadata Labels and Annotations gets propagated to the ThanosRuler pods.\n    ##\n    podMetadata: {}\n\n    ## Image of ThanosRuler\n    ##\n    image:\n      registry: quay.io\n      repository: thanos/thanos\n      tag: v0.32.2\n      sha: \"\"\n\n    ## Namespaces to be selected for PrometheusRules discovery.\n    ## If nil, select own namespace. Namespaces to be selected for ServiceMonitor discovery.\n    ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#namespaceselector for usage\n    ##\n    ruleNamespaceSelector: {}\n\n    ## If true, a nil or {} value for thanosRuler.thanosRulerSpec.ruleSelector will cause the\n    ## prometheus resource to be created with selectors based on values in the helm deployment,\n    ## which will also match the PrometheusRule resources created\n    ##\n    ruleSelectorNilUsesHelmValues: true\n\n    ## PrometheusRules to be selected for target discovery.\n    ## If {}, select all PrometheusRules\n    ##\n    ruleSelector: {}\n    ## Example which select all PrometheusRules resources\n    ## with label \"prometheus\" with values any of \"example-rules\" or \"example-rules-2\"\n    # ruleSelector:\n    #   matchExpressions:\n    #     - key: prometheus\n    #       operator: In\n    #       values:\n    #         - example-rules\n    #         - example-rules-2\n    #\n    ## Example which select all PrometheusRules resources with label \"role\" set to \"example-rules\"\n    # ruleSelector:\n    #   matchLabels:\n    #     role: example-rules\n\n    ## Define Log Format\n    # Use logfmt (default) or json logging\n    logFormat: logfmt\n\n    ## Log level for ThanosRuler to be configured with.\n    ##\n    logLevel: info\n\n    ## Size is the expected size of the thanosRuler cluster. The controller will eventually make the size of the\n    ## running cluster equal to the expected size.\n    replicas: 1\n\n    ## Time duration ThanosRuler shall retain data for. Default is '24h', and must match the regular expression\n    ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).\n    ##\n    retention: 24h\n\n    ## Interval between consecutive evaluations.\n    ##\n    evaluationInterval: \"\"\n\n    ## Storage is the definition of how storage will be used by the ThanosRuler instances.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md\n    ##\n    storage: {}\n    # volumeClaimTemplate:\n    #   spec:\n    #     storageClassName: gluster\n    #     accessModes: [\"ReadWriteOnce\"]\n    #     resources:\n    #       requests:\n    #         storage: 50Gi\n    #   selector: {}\n\n    ## AlertmanagerConfig define configuration for connecting to alertmanager.\n    ## Only available with Thanos v0.10.0 and higher. Maps to the alertmanagers.config Thanos Ruler arg.\n    alertmanagersConfig: {}\n    #   - api_version: v2\n    #     http_config:\n    #       basic_auth:\n    #         username: some_user\n    #         password: some_pass\n    #     static_configs:\n    #       - alertmanager.thanos.io\n    #     scheme: http\n    #     timeout: 10s\n\n    ## DEPRECATED. Define URLs to send alerts to Alertmanager. For Thanos v0.10.0 and higher, alertmanagersConfig should be used instead.\n    ## Note: this field will be ignored if alertmanagersConfig is specified. Maps to the alertmanagers.url Thanos Ruler arg.\n    # alertmanagersUrl:\n\n    ## The external URL the Thanos Ruler instances will be available under. This is necessary to generate correct URLs. This is necessary if Thanos Ruler is not served from root of a DNS name. string false\n    ##\n    externalPrefix:\n\n    ## The route prefix ThanosRuler registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,\n    ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.\n    ##\n    routePrefix: /\n\n    ## ObjectStorageConfig configures object storage in Thanos. Alternative to\n    ## ObjectStorageConfigFile, and lower order priority.\n    objectStorageConfig: {}\n\n    ## ObjectStorageConfigFile specifies the path of the object storage configuration file.\n    ## When used alongside with ObjectStorageConfig, ObjectStorageConfigFile takes precedence.\n    objectStorageConfigFile: \"\"\n\n    ## QueryEndpoints defines Thanos querier endpoints from which to query metrics.\n    ## Maps to the --query flag of thanos ruler.\n    queryEndpoints: []\n\n    ## Define configuration for connecting to thanos query instances. If this is defined, the queryEndpoints field will be ignored.\n    ## Maps to the query.config CLI argument. Only available with thanos v0.11.0 and higher.\n    queryConfig: {}\n\n    ## Labels configure the external label pairs to ThanosRuler. A default replica\n    ## label `thanos_ruler_replica` will be always added as a label with the value\n    ## of the pod's name and it will be dropped in the alerts.\n    labels: {}\n\n    ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.\n    ##\n    paused: false\n\n    ## Define which Nodes the Pods are scheduled on.\n    ## ref: https://kubernetes.io/docs/user-guide/node-selection/\n    ##\n    nodeSelector: {}\n\n    ## Define resources requests and limits for single Pods.\n    ## ref: https://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n    # requests:\n    #   memory: 400Mi\n\n    ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.\n    ## The default value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n    ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n    ## The value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured.\n    ##\n    podAntiAffinity: \"\"\n\n    ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n    ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n    ##\n    podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n    ## Assign custom affinity rules to the thanosRuler instance\n    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n    ##\n    affinity: {}\n    # nodeAffinity:\n    #   requiredDuringSchedulingIgnoredDuringExecution:\n    #     nodeSelectorTerms:\n    #     - matchExpressions:\n    #       - key: kubernetes.io/e2e-az-name\n    #         operator: In\n    #         values:\n    #         - e2e-az1\n    #         - e2e-az2\n\n    ## If specified, the pod's tolerations.\n    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/\n    ##\n    tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule\"\n\n    ## If specified, the pod's topology spread constraints.\n    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/\n    ##\n    topologySpreadConstraints: []\n    # - maxSkew: 1\n    #   topologyKey: topology.kubernetes.io/zone\n    #   whenUnsatisfiable: DoNotSchedule\n    #   labelSelector:\n    #     matchLabels:\n    #       app: thanos-ruler\n\n    ## SecurityContext holds pod-level security attributes and common container settings.\n    ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false\n    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/\n    ##\n    securityContext:\n      runAsGroup: 2000\n      runAsNonRoot: true\n      runAsUser: 1000\n      fsGroup: 2000\n      seccompProfile:\n        type: RuntimeDefault\n\n    ## ListenLocal makes the ThanosRuler server listen on loopback, so that it does not bind against the Pod IP.\n    ## Note this is only for the ThanosRuler UI, not the gossip communication.\n    ##\n    listenLocal: false\n\n    ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an ThanosRuler pod.\n    ##\n    containers: []\n\n    # Additional volumes on the output StatefulSet definition.\n    volumes: []\n\n    # Additional VolumeMounts on the output StatefulSet definition.\n    volumeMounts: []\n\n    ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes\n    ## (permissions, dir tree) on mounted volumes before starting prometheus\n    initContainers: []\n\n    ## Priority class assigned to the Pods\n    ##\n    priorityClassName: \"\"\n\n    ## PortName to use for ThanosRuler.\n    ##\n    portName: \"web\"\n\n  ## ExtraSecret can be used to store various data in an extra secret\n  ## (use it for example to store hashed basic auth credentials)\n  extraSecret:\n    ## if not set, name will be auto generated\n    # name: \"\"\n    annotations: {}\n    data: {}\n  #   auth: |\n  #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0\n  #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.\n\n## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.\n##\ncleanPrometheusOperatorObjectNames: false\n\n## Extra manifests to deploy as an array\nextraManifests: []\n  # - apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n  #     extra-data: \"value\"\n"
            ],
            "verify": false,
            "version": "51.0.3",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "kubernetes_namespace.prometheus_ns"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "jenkins_ns",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "jenkins",
            "metadata": [
              {
                "annotations": {},
                "generate_name": "",
                "generation": 0,
                "labels": {},
                "name": "jenkins",
                "resource_version": "422445",
                "uid": "9ed18422-4fea-4bf6-a528-1cc00e9851c7"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "prometheus_ns",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "prometheus",
            "metadata": [
              {
                "annotations": {
                  "name": "prometheus"
                },
                "generate_name": "",
                "generation": 0,
                "labels": {
                  "role": "prometheus"
                },
                "name": "prometheus",
                "resource_version": "432876",
                "uid": "eda4160e-7203-4816-b672-2c5a8c9a9b8b"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ=="
        }
      ]
    }
  ],
  "check_results": null
}
